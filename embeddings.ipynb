{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Тестирование эмбедингов"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rusvectors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\r\n",
    "import zipfile\r\n",
    "import wget\r\n",
    "from preproccess_text import tag_ud"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "сorpus_file = \"./dataset/corpus.txt\"\r\n",
    "# with open(corpus_file, \"wb\") as f:\r\n",
    "#     pickle.dump(corpus, f)\r\n",
    "\r\n",
    "with open(сorpus_file, \"rb\") as f:   # Unpickling\r\n",
    "    corpus = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus = []\r\n",
    "stops = set(stopwords.words('russian'))\r\n",
    "root_path = 'C:\\\\Users\\\\edbon\\\\devproj\\\\faiky-tails\\\\dataset\\\\raw\\\\'\r\n",
    "modelfile = './udpipe_syntagrus.model'\r\n",
    "with open(root_path + '001 Арысь - поле.txt', 'r', encoding='utf-8') as f:\r\n",
    "    doc = tag_ud(text=f.read(), modelfile=modelfile, stop_words=stops, keep_pos=True, keep_punct=False)\r\n",
    "corpus.append(doc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_path = 'c:/models/rusvector/'\r\n",
    "model_file = model_path + '180.zip'\r\n",
    "if not os.path.exists(model_file):\r\n",
    "    model_url = 'http://vectors.nlpl.eu/repository/11/180.zip'\r\n",
    "    m = wget.download(model_url, out=model_path)\r\n",
    "    model_file = model_path + model_url.split('/')[-1]\r\n",
    "\r\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\r\n",
    "    stream = archive.open('model.bin')\r\n",
    "    model_rv = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(stops)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'почти', 'над', 'тогда', 'с', 'при', 'вдруг', 'теперь', 'без', 'после', 'про', 'сейчас', 'как', 'по', 'кто', 'есть', 'тем', 'вот', 'что', 'мой', 'какая', 'она', 'тут', 'ведь', 'нет', 'сам', 'всегда', 'всю', 'впрочем', 'перед', 'потому', 'зачем', 'свою', 'и', 'а', 'так', 'чтобы', 'не', 'да', 'была', 'тебя', 'ему', 'вам', 'может', 'конечно', 'вы', 'со', 'из', 'то', 'него', 'чем', 'иногда', 'более', 'мне', 'этого', 'на', 'один', 'ее', 'разве', 'того', 'два', 'эту', 'об', 'нее', 'всех', 'там', 'под', 'будет', 'были', 'его', 'все', 'ли', 'ничего', 'они', 'уж', 'чтоб', 'опять', 'меня', 'эти', 'всего', 'куда', 'моя', 'лучше', 'много', 'у', 'вас', 'для', 'мы', 'или', 'ней', 'во', 'чего', 'надо', 'какой', 'уже', 'чуть', 'от', 'хорошо', 'еще', 'совсем', 'был', 'никогда', 'тот', 'он', 'можно', 'ни', 'этой', 'ну', 'нас', 'о', 'этом', 'до', 'себя', 'через', 'раз', 'быть', 'больше', 'ж', 'за', 'бы', 'но', 'потом', 'ты', 'хоть', 'между', 'этот', 'даже', 'когда', 'будто', 'такой', 'нибудь', 'том', 'наконец', 'им', 'них', 'я', 'где', 'тоже', 'другой', 'если', 'ей', 'в', 'ним', 'только', 'их', 'себе', 'было', 'три', 'здесь', 'к', 'же', 'нельзя'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# надо суффикс проставлять для слова с частеречием\r\n",
    "\r\n",
    "bad_words = set()\r\n",
    "for word in corpus[0]:\r\n",
    "    try:\r\n",
    "        model_rv[word]\r\n",
    "    except KeyError as err:\r\n",
    "        bad_words.add(word)\r\n",
    "print(\"не в словаре:\", bad_words)\r\n",
    "\r\n",
    "for word in bad_words:\r\n",
    "    w = word.split('_')[0]\r\n",
    "    \r\n",
    "    if w in stops:\r\n",
    "        print(\"стоп слова:\", word, end=' ')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'арысь_NOUN', 'весь_DET', 'невзлюбить_PROPN', 'стариковый_ADJ', 'свой_DET', 'падчерицыный_ADV', 'вместо_ADP', 'никто_PRON'}\n",
      "арысь\n",
      "весь\n",
      "невзлюбить\n",
      "стариковый\n",
      "свой\n",
      "падчерицыный\n",
      "вместо\n",
      "никто\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Результаты:\r\n",
    "\r\n",
    "* в готовом эмбединге отсутствуют часть слов из корпуса, надо смотреть возможности дообучения"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Исследование Word2Vec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import numpy as np\r\n",
    "import timeit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "f = lambda x: np.exp(x)\r\n",
    "vf = np.vectorize(f)\r\n",
    "\r\n",
    "a = np.array([3.0, 1.0, -3.0])\r\n",
    "softmax = np.exp(a) / np.exp(a).sum()\r\n",
    "print(type(softmax), softmax)\r\n",
    "\r\n",
    "n = 10000\r\n",
    "t1 = timeit.timeit('np.exp(a) / np.exp(a).sum()', 'from __main__ import a, divider, np', number=n)\r\n",
    "t2 = timeit.timeit('vf(a) / np.exp(a).sum()', 'from __main__ import a, divider, np, vf', number=n)\r\n",
    "t3 = timeit.timeit('f(a) / np.exp(a).sum()', 'from __main__ import a, divider, np, f', number=n)\r\n",
    "\r\n",
    "\r\n",
    "print(\"direct np func=\", round(t1, 3))\r\n",
    "print(\"np vectorize=\", round(t2, 3))\r\n",
    "print(\"direct my f on np func=\", round(t3, 3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'> [0.87887824 0.11894324 0.00217852]\n",
      "direct np func= 0.031\n",
      "np vectorize= 0.128\n",
      "direct my f on np func= 0.029\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('tf': conda)"
  },
  "interpreter": {
   "hash": "2d3a13fa35461c6d7f441fbb3d93fc4f14860aa527e8f914775d5e57b8364c02"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}