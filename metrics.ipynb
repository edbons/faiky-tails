{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import re\r\n",
    "import gensim\r\n",
    "from nltk.corpus import stopwords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preproccess_text(text: str) -> list:\r\n",
    "    text = re.sub('[«»()!,;.\\s-]', ' ', text)\r\n",
    "    return gensim.utils.simple_preprocess(text, min_len=2, max_len=15)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "root_path = 'C:\\\\Users\\\\edbon\\\\devproj\\\\faiky-tails\\\\dataset\\\\raw\\\\'\r\n",
    "corpus = []\r\n",
    "\r\n",
    "for _, dirs, files in os.walk(root_path):\r\n",
    "    for file in files:\r\n",
    "        with open(root_path + file, 'r', encoding='utf-8') as f:\r\n",
    "            doc = preproccess_text(f.read())\r\n",
    "        corpus.append(doc) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(corpus[0][:50])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NLTK"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk, \r\n",
    "import re\r\n",
    "import pprint\r\n",
    "from nltk import word_tokenize, FreqDist, bigrams\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nltk.download('averaged_perceptron_tagger_ru')\r\n",
    "nltk.download('rte')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text1 = re.sub(\"[\\n]\", ' ', text1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokens = word_tokenize(text1, language='russian')\r\n",
    "sents = nltk.sent_tokenize(text1, language='russian')\r\n",
    "\r\n",
    "sents_tokens = [word_tokenize(sent) for sent in sents]\r\n",
    "\r\n",
    "print(len(sents))\r\n",
    "print(sents_tokens)\r\n",
    "# pprint.pprint(sents_tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text simple analyze"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text = nltk.Text(tokens=tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(text.collocations())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(list(bigrams((text))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    " fdist1 = FreqDist(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(fdist1.most_common(30))\r\n",
    "fdist1.plot(50, cumulative=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## POS TAGS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sent_tags = nltk.pos_tag_sents(sents_tokens, lang='rus')\r\n",
    "pprint.pprint(sent_tags)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tags = nltk.pos_tag(tokens, lang='rus')\r\n",
    "print(tags)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RTE extractor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def rte_features(rtepair):\r\n",
    "    extractor = nltk.RTEFeatureExtractor(rtepair)\r\n",
    "    features = {}\r\n",
    "    features['word_overlap'] = len(extractor.overlap('word'))\r\n",
    "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\r\n",
    "    features['ne_overlap'] = len(extractor.overlap('ne'))\r\n",
    "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\r\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\r\n",
    "extractor = nltk.RTEFeatureExtractor(rtepair)\r\n",
    "print(extractor.overlap(toktype='ne'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confusion matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.corpus import brown\r\n",
    "t2 = nltk.UnigramTagger(brown.tagged_sents(categories='editorial'))\r\n",
    "\r\n",
    "def tag_list(tagged_sents):\r\n",
    "    return [tag for sent in tagged_sents for (word, tag) in sent]\r\n",
    "\r\n",
    "def apply_tagger(tagger, corpus):\r\n",
    "    return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]\r\n",
    "\r\n",
    "gold = tag_list(brown.tagged_sents(categories='editorial'))\r\n",
    "test = tag_list(apply_tagger(t2, brown.tagged_sents(categories='editorial')))\r\n",
    "cm = nltk.ConfusionMatrix(gold, test)\r\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nltk.downloader.download_gui()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsing With Context Free Grammar"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Для русского не найдены граммеры\r\n",
    "grammar = nltk.parse_cfg(\"\"\"\r\n",
    "  S -> NP VP\r\n",
    "  VP -> V NP | V NP PP\r\n",
    "  PP -> P NP\r\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\r\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\r\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\r\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\r\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\r\n",
    "  \"\"\")\r\n",
    "rd_parser = nltk.RecursiveDescentParser(TestGrammar)\r\n",
    "for tree in rd_parser.parse(sents_tokens[0]):\r\n",
    "    print(tree)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gensim"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.models import doc2vec\r\n",
    "from gensim import similarities\r\n",
    "from gensim import corpora\r\n",
    "import random\r\n",
    "from copy import copy\r\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dictionary = corpora.Dictionary(corpus)\r\n",
    "dictionary.save('./dataset/tails.dict')\r\n",
    "print(dictionary.num_docs)\r\n",
    "print(dictionary.num_nnz)\r\n",
    "print(dictionary.num_pos)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dictionary.filter_extremes()\r\n",
    "print(dictionary.num_docs)\r\n",
    "print(dictionary.num_nnz)\r\n",
    "print(dictionary.num_pos)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BoW, TfIdf, LSI"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def most_simillar(model, target, corpus, documents):    \r\n",
    "    vec_bow = dictionary.doc2bow(target)\r\n",
    "    vec = model[vec_bow]\r\n",
    "    # print(vec_lsi)\r\n",
    "\r\n",
    "    index = similarities.MatrixSimilarity(model[corpus])\r\n",
    "    sims = index[vec]  # perform a similarity query against the corpus\r\n",
    "    # print(list(enumerate(sims)))\r\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\r\n",
    "    print(\"new vector:\", target)\r\n",
    "    print()\r\n",
    "    for doc_position, doc_score in sims[:1]:\r\n",
    "        print(doc_score, documents[doc_position])\r\n",
    "    return ' '.join(documents[doc_position])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# порядок слов не учитывается в BoW\r\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in corpus]\r\n",
    "corpora.MmCorpus.serialize('./dataset/tails.mm', bow_corpus)  # store to disk, for later use"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_lsi = gensim.models.LsiModel(bow_corpus, id2word=dictionary, num_topics=300)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#doc = preproccess_text(\"Царь лежал на диване и потягивал смузи. В дом зашел Кащей и начал разговор.\")\r\n",
    "doc = corpus[0][:20]\r\n",
    "doc_pred = most_simillar(model=model_lsi, target=doc, corpus=bow_corpus, documents=corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_tfidf = gensim.models.TfidfModel(bow_corpus, normalize=True)\r\n",
    "tf_corpus = model_tfidf[bow_corpus]\r\n",
    "model_lsi_tf = gensim.models.LsiModel(corpus=tf_corpus, num_topics=300, id2word=dictionary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# doc = \"Царь лежал на диване и потягивал смузи. В дом зашел Кащей и начал разговор.\".lower().split()\r\n",
    "\r\n",
    "doc = corpus[0].copy()\r\n",
    "doc = random.choices(doc, k=30)\r\n",
    "\r\n",
    "doc_pred = most_simillar(model=model_lsi_tf, target=doc, corpus=tf_corpus, documents=corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# doc = \"Царь лежал на диване и потягивал смузи. В дом зашел Кащей и начал разговор.\".lower().split()\r\n",
    "doc = corpus[0][:20]\r\n",
    "doc_pred = most_simillar(model=model_tfidf, target=doc, corpus=tf_corpus, documents=corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word2Vec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately, the model is unable to infer vectors for unfamiliar words. This is one limitation of Word2Vec: if this limitation matters to you, check out the FastText model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = gensim.models.Word2Vec(epochs=20, vector_size=100, window=5, min_count=1)\r\n",
    "model.build_vocab(corpus)\r\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyemd import emd\r\n",
    "word_vectors = model.wv\r\n",
    "sentence1 = corpus[0][:10]\r\n",
    "sentence2 = corpus[1][:20]\r\n",
    "print(\"sentence1=\", sentence1)\r\n",
    "print(\"sentence2=\", sentence2)\r\n",
    "similarity = word_vectors.wmdistance(sentence1, sentence2)\r\n",
    "print(f\"wmdistance={similarity:.4f}\")\r\n",
    "\r\n",
    "similarity = word_vectors.n_similarity(sentence1, sentence2)\r\n",
    "print(f\"n_similarity={similarity:.4f}\")\r\n",
    "0.7067\r\n",
    "\r\n",
    "distance = word_vectors.distance('царь', 'кощей')\r\n",
    "print(f\"'царь - кощей' distance={distance:.1f}\")\r\n",
    "\r\n",
    "vector = word_vectors.get_vector('кощей', norm=True)\r\n",
    "print('кощей', vector.shape)\r\n",
    "# (100,)\r\n",
    "\r\n",
    "print(f'\"баба старик ведро\" doesnt_match={word_vectors.doesnt_match(\"баба старик ведро\".split())}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.predict_output_word(['баба'], topn=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for index, word in enumerate(model.wv.index_to_key):\r\n",
    "    if index == 10:\r\n",
    "        break\r\n",
    "    print(f\"word #{index}/{len(model.wv.index_to_key)} is {word}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\r\n",
    "from sklearn.manifold import TSNE                   # final reduction\r\n",
    "import numpy as np                                  # array handling\r\n",
    "\r\n",
    "\r\n",
    "def reduce_dimensions(model):\r\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\r\n",
    "\r\n",
    "    # extract the words & their vectors, as numpy arrays\r\n",
    "    vectors = np.asarray(model.wv.vectors)\r\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\r\n",
    "\r\n",
    "    # reduce using t-SNE\r\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\r\n",
    "    vectors = tsne.fit_transform(vectors)\r\n",
    "\r\n",
    "    x_vals = [v[0] for v in vectors]\r\n",
    "    y_vals = [v[1] for v in vectors]\r\n",
    "    return x_vals, y_vals, labels\r\n",
    "\r\n",
    "\r\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\r\n",
    "\r\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\r\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\r\n",
    "    import plotly.graph_objs as go\r\n",
    "\r\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\r\n",
    "    data = [trace]\r\n",
    "\r\n",
    "    if plot_in_notebook:\r\n",
    "        init_notebook_mode(connected=True)\r\n",
    "        iplot(data, filename='word-embedding-plot')\r\n",
    "    else:\r\n",
    "        plot(data, filename='word-embedding-plot.html')\r\n",
    "\r\n",
    "\r\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\r\n",
    "    import matplotlib.pyplot as plt\r\n",
    "    import random\r\n",
    "\r\n",
    "    random.seed(0)\r\n",
    "\r\n",
    "    plt.figure(figsize=(12, 12))\r\n",
    "    plt.scatter(x_vals, y_vals)\r\n",
    "\r\n",
    "    #\r\n",
    "    # Label randomly subsampled 25 data points\r\n",
    "    #\r\n",
    "    indices = list(range(len(labels)))\r\n",
    "    selected_indices = random.sample(indices, 25)\r\n",
    "    for i in selected_indices:\r\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\r\n",
    "\r\n",
    "try:\r\n",
    "    get_ipython()\r\n",
    "except Exception:\r\n",
    "    plot_function = plot_with_matplotlib\r\n",
    "else:\r\n",
    "    plot_function = plot_with_plotly\r\n",
    "\r\n",
    "plot_function(x_vals, y_vals, labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### W2V Phrases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.models import Phrases\r\n",
    "\r\n",
    "bigram_transformer = Phrases(corpus)\r\n",
    "\r\n",
    "model_bigram = gensim.models.Word2Vec(bigram_transformer[corpus], min_count=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x_vals, y_vals, labels = reduce_dimensions(model_bigram)\r\n",
    "plot_function(x_vals, y_vals, labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ruscorpora word2vec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ruscorpora_vectors = gensim.downloader.load('word2vec-ruscorpora-300')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list(ruscorpora_vectors.key_to_index.keys())[:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import zipfile\r\n",
    "import wget\r\n",
    "\r\n",
    "model_path = 'c:/models/rusvector/'\r\n",
    "model_url = 'http://vectors.nlpl.eu/repository/11/180.zip'\r\n",
    "m = wget.download(model_url, out=model_path)\r\n",
    "model_file = model_path + model_url.split('/')[-1]\r\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\r\n",
    "    stream = archive.open('model.bin')\r\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list(model.key_to_index.keys())[:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# надо суффикс проставлять для слова с частеречием\r\n",
    "vec1 = model.most_similar(positive=['один_NUM'])\r\n",
    "print(vec1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Doc2Vec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import logging\r\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data = corpus[:round(len(corpus) * 0.8)]\r\n",
    "test_data = corpus[round(len(corpus) * 0.8):]\r\n",
    "print(len(train_data), len(test_data))\r\n",
    "\r\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(tokens, [i]) for i, tokens in enumerate(train_data)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_corpus[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_d2v = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=1, epochs=40)\r\n",
    "model_d2v.build_vocab(train_corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_d2v.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vector = model_d2v.infer_vector(corpus[0][:10])\r\n",
    "print(vector)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ranks = []\r\n",
    "second_ranks = []\r\n",
    "for doc_id in range(len(train_corpus)):\r\n",
    "    inferred_vector = model_d2v.infer_vector(train_corpus[doc_id].words)\r\n",
    "    sims = model_d2v.dv.most_similar([inferred_vector], topn=len(model_d2v.dv))\r\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\r\n",
    "    ranks.append(rank)\r\n",
    "\r\n",
    "    second_ranks.append(sims[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import collections\r\n",
    "\r\n",
    "counter = collections.Counter(ranks)\r\n",
    "print(counter)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc_id = random.choice(range(len(train_corpus)))\r\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\r\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\r\n",
    "inferred_vector = model_d2v.infer_vector(train_corpus[doc_id].words)\r\n",
    "sims = model_d2v.dv.most_similar([inferred_vector], topn=len(model_d2v.dv))\r\n",
    "for label, index in sims[:3]:\r\n",
    "    print(f'{label} {index:2.2%}: {\" \".join(train_corpus[label].words)}')\r\n",
    "    print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### !! убрать надо ударения в словах"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc_id = random.randint(0, len(test_data) - 1)\r\n",
    "inferred_vector = model_d2v.infer_vector(test_data[doc_id])\r\n",
    "sims = model_d2v.dv.most_similar([inferred_vector], topn=len(model_d2v.dv))\r\n",
    "\r\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_data[doc_id])))\r\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model_d2v)\r\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\r\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FastText"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Особенности:\r\n",
    "\r\n",
    "* фастекст определяет векторы подстрок входящих в слово и выдает суммарный вектор на их основе. \r\n",
    "* В отличии от w2v может обрабатывать слова не из словаря если хотя бы одна из подстрок этого слова (к примеру корень) есть в векторном пространстве"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_ft = gensim.models.FastText(vector_size=100)\r\n",
    "\r\n",
    "# build the vocabulary\r\n",
    "model_ft.build_vocab(corpus)\r\n",
    "\r\n",
    "# train the model\r\n",
    "model_ft.train(\r\n",
    "    corpus, \r\n",
    "    epochs=model_ft.epochs,\r\n",
    "    total_examples=model_ft.corpus_count, \r\n",
    "    total_words=model_ft.corpus_total_words,\r\n",
    ")\r\n",
    "\r\n",
    "print(model_ft)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wv = model_ft.wv\r\n",
    "print(wv)\r\n",
    "\r\n",
    "print('путин' in wv.key_to_index)\r\n",
    "try:\r\n",
    "    word = 'путин'\r\n",
    "    print(wv[word])\r\n",
    "\r\n",
    "except KeyError as err:\r\n",
    "    print(err)\r\n",
    "\r\n",
    "print(wv.similarity(\"путь\", \"путин\"))\r\n",
    "print(wv.most_similar(\"путь\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sentence1 = corpus[0][:10]\r\n",
    "sentence2 = corpus[10][:20]\r\n",
    "print(wv.n_similarity(sentence1, sentence2))\r\n",
    "distance = wv.wmdistance(sentence1, sentence2)\r\n",
    "print(f\"Word Movers Distance is {distance} (lower means closer)\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Movers Distance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Word Movers Distance on fastText')\r\n",
    "sentence1 = corpus[0][:10]\r\n",
    "sentence2 = preproccess_text('сегодня в новостях опять кого то убили ни за что, а ведь он мог учиться в университете')\r\n",
    "print('sentence1', sentence1)\r\n",
    "print('sentence2', sentence2)\r\n",
    "distance = wv.wmdistance(sentence1, sentence2)\r\n",
    "print(f\"Word Movers Distance is {distance} (lower means closer)\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word_vectors_d2v = model_d2v.wv\r\n",
    "print('Word Movers Distance on Doc2Vec')\r\n",
    "sentence1 = corpus[0][:10]\r\n",
    "# sentence2 = preproccess_text('сегодня в новостях опять кого то убили ни за что, а ведь он мог учиться в университете')\r\n",
    "sentence2 = corpus[0][5:15]\r\n",
    "print('sentence1', sentence1)\r\n",
    "print('sentence2', sentence2)\r\n",
    "distance = word_vectors_d2v.wmdistance(sentence1, sentence2)\r\n",
    "print(f\"Word Movers Distance is {distance} (lower means closer)\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Soft Cosine Measure"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.similarities import SparseTermSimilarityMatrix, WordEmbeddingSimilarityIndex, SoftCosineSimilarity"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Word Movers Distance on Doc2Vec')\r\n",
    "sentence1 = corpus[0][:20]\r\n",
    "sentence2 = corpus[0][20:40]\r\n",
    "sentence3 = preproccess_text('сегодня в новостях опять кого то убили ни за что, а ведь он мог учиться в университете')\r\n",
    "print('sentence1', sentence1)\r\n",
    "print('sentence2', sentence2)\r\n",
    "print('sentence3', sentence3)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = gensim.models.Word2Vec(epochs=20, vector_size=100, window=5, min_count=1)\r\n",
    "model.build_vocab(corpus)\r\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "termsim_index = WordEmbeddingSimilarityIndex(model.wv)\r\n",
    "dictionary = gensim.corpora.Dictionary(corpus)\r\n",
    "bow_corpus = [dictionary.doc2bow(document) for document in corpus]\r\n",
    "similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sent_1 = dictionary.doc2bow(sentence1)\r\n",
    "sent_2 = dictionary.doc2bow(sentence2)\r\n",
    "sent_3 = dictionary.doc2bow(sentence3)\r\n",
    "print(sentence1, '\\n', sentence2)\r\n",
    "similarity = similarity_matrix.inner_product(sent_1, sent_2, normalized=(True, True))\r\n",
    "print('similarity = %.4f' % similarity)\r\n",
    "print()\r\n",
    "print(sentence1, '\\n', sentence3)\r\n",
    "similarity = similarity_matrix.inner_product(sent_1, sent_3, normalized=(True, True))\r\n",
    "print('similarity = %.4f' % similarity)\r\n",
    "print()\r\n",
    "print(sentence1, '\\n', sentence1)\r\n",
    "similarity = similarity_matrix.inner_product(sent_1, sent_1, normalized=(True, True))\r\n",
    "print('similarity = %.4f' % similarity)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pickle\r\n",
    "import zipfile\r\n",
    "import wget\r\n",
    "from preproccess_text import tag_ud"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "сorpus_file = \"./dataset/corpus.txt\"\r\n",
    "# with open(corpus_file, \"wb\") as f:\r\n",
    "#     pickle.dump(corpus, f)\r\n",
    "\r\n",
    "with open(сorpus_file, \"rb\") as f:   # Unpickling\r\n",
    "    corpus = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "corpus = []\r\n",
    "stops = set(stopwords.words('russian'))\r\n",
    "root_path = 'C:\\\\Users\\\\edbon\\\\devproj\\\\faiky-tails\\\\dataset\\\\raw\\\\'\r\n",
    "modelfile = './udpipe_syntagrus.model'\r\n",
    "with open(root_path + '001 Арысь - поле.txt', 'r', encoding='utf-8') as f:\r\n",
    "    doc = tag_ud(text=f.read(), modelfile=modelfile, stop_words=stops, keep_pos=True, keep_punct=False)\r\n",
    "corpus.append(doc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model_path = 'c:/models/rusvector/'\r\n",
    "model_file = model_path + '180.zip'\r\n",
    "if not os.path.exists(model_file):\r\n",
    "    model_url = 'http://vectors.nlpl.eu/repository/11/180.zip'\r\n",
    "    m = wget.download(model_url, out=model_path)\r\n",
    "    model_file = model_path + model_url.split('/')[-1]\r\n",
    "\r\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\r\n",
    "    stream = archive.open('model.bin')\r\n",
    "    model_rv = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print(stops)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'почти', 'над', 'тогда', 'с', 'при', 'вдруг', 'теперь', 'без', 'после', 'про', 'сейчас', 'как', 'по', 'кто', 'есть', 'тем', 'вот', 'что', 'мой', 'какая', 'она', 'тут', 'ведь', 'нет', 'сам', 'всегда', 'всю', 'впрочем', 'перед', 'потому', 'зачем', 'свою', 'и', 'а', 'так', 'чтобы', 'не', 'да', 'была', 'тебя', 'ему', 'вам', 'может', 'конечно', 'вы', 'со', 'из', 'то', 'него', 'чем', 'иногда', 'более', 'мне', 'этого', 'на', 'один', 'ее', 'разве', 'того', 'два', 'эту', 'об', 'нее', 'всех', 'там', 'под', 'будет', 'были', 'его', 'все', 'ли', 'ничего', 'они', 'уж', 'чтоб', 'опять', 'меня', 'эти', 'всего', 'куда', 'моя', 'лучше', 'много', 'у', 'вас', 'для', 'мы', 'или', 'ней', 'во', 'чего', 'надо', 'какой', 'уже', 'чуть', 'от', 'хорошо', 'еще', 'совсем', 'был', 'никогда', 'тот', 'он', 'можно', 'ни', 'этой', 'ну', 'нас', 'о', 'этом', 'до', 'себя', 'через', 'раз', 'быть', 'больше', 'ж', 'за', 'бы', 'но', 'потом', 'ты', 'хоть', 'между', 'этот', 'даже', 'когда', 'будто', 'такой', 'нибудь', 'том', 'наконец', 'им', 'них', 'я', 'где', 'тоже', 'другой', 'если', 'ей', 'в', 'ним', 'только', 'их', 'себе', 'было', 'три', 'здесь', 'к', 'же', 'нельзя'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# надо суффикс проставлять для слова с частеречием\r\n",
    "\r\n",
    "bad_words = set()\r\n",
    "for word in corpus[0]:\r\n",
    "    try:\r\n",
    "        model_rv[word]\r\n",
    "    except KeyError as err:\r\n",
    "        bad_words.add(word)\r\n",
    "print(bad_words)\r\n",
    "\r\n",
    "for word in bad_words:\r\n",
    "    w = word.split('_')[0]\r\n",
    "    \r\n",
    "    if w in stops:\r\n",
    "        print(word)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'арысь_NOUN', 'весь_DET', 'невзлюбить_PROPN', 'стариковый_ADJ', 'свой_DET', 'падчерицыный_ADV', 'вместо_ADP', 'никто_PRON'}\n",
      "арысь\n",
      "весь\n",
      "невзлюбить\n",
      "стариковый\n",
      "свой\n",
      "падчерицыный\n",
      "вместо\n",
      "никто\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('tf': conda)"
  },
  "interpreter": {
   "hash": "2d3a13fa35461c6d7f441fbb3d93fc4f14860aa527e8f914775d5e57b8364c02"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}