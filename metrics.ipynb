{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import re\r\n",
    "import gensim\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from preproccess_text import clean_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preproccess_text(text: str) -> list:\r\n",
    "    text = re.sub('[«»()!,;:.\\s-]', ' ', text)\r\n",
    "    return gensim.utils.simple_preprocess(text, min_len=2, max_len=15)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "root_path = 'C:\\\\Users\\\\edbon\\\\devproj\\\\faiky-tails\\\\dataset\\\\raw\\\\'\r\n",
    "corpus = []\r\n",
    "\r\n",
    "for _, dirs, files in os.walk(root_path):\r\n",
    "    for file in files:\r\n",
    "        with open(root_path + file, 'r', encoding='utf-8') as f:\r\n",
    "            doc = clean_text(f.read())\r\n",
    "        corpus.append(doc.split()) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "corp_size = [len(doc) for doc in corpus]\r\n",
    "print(corp_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[242, 118, 319, 443, 104, 621, 603, 353, 205, 69, 177, 479, 673, 96, 523, 427, 885, 728, 158, 1207, 265, 624, 202, 478, 160, 97, 38, 183, 291, 533, 456, 611, 399, 464, 285, 649, 237, 352, 168, 521, 472, 70, 135, 614, 296, 263, 205, 740, 710, 159, 149, 756, 152, 64, 116, 335, 75, 703, 553, 168, 96, 114, 125, 1873, 76, 882, 110, 97, 612, 220, 618, 197, 206, 478, 273, 2270, 268, 63, 366, 246, 4687, 127, 1093, 75, 340, 744, 3803, 991, 98, 213, 338, 462, 1959, 87, 289, 2298, 483, 503, 608, 338, 385, 510, 474, 235, 589, 2434, 963, 2375, 927, 803, 3893, 355, 1478, 610, 3335, 735, 255, 813, 431, 1524, 1995, 3578, 2548, 1932, 364, 301, 277, 242, 372, 891, 1570, 483, 2625, 476, 368, 906, 411, 457, 840, 494, 438, 823, 301, 235, 1502, 222, 688, 1072, 370, 634, 652, 1328, 364, 1608, 1238, 1902, 2295, 190, 1246, 74, 1181, 1453, 765, 1002, 500, 102, 611, 197, 118, 234, 1418, 940, 145, 334, 110, 1084, 208, 536, 455, 152, 226, 307, 104, 400, 85, 369, 928, 398, 131, 429, 102, 317, 529, 181, 227, 239, 1473, 305, 235, 59, 779, 558, 2369, 56, 293, 156, 169, 268, 169, 21, 78, 152, 584, 276, 2364, 226, 196, 2257, 763, 317, 421, 392, 175, 255, 180, 684, 547, 373, 1100, 246, 199, 353, 129, 213, 65, 178, 316, 230, 33, 198, 957, 41, 25, 137, 77, 396, 320, 88, 317, 134, 324, 193, 384, 293, 194, 251, 871, 82, 249, 587, 654, 242, 332, 100, 552, 82, 37, 89, 915, 267, 1166, 182, 1339, 1201, 293, 873, 346, 124, 2495, 976, 1981, 1007, 1386, 898, 750, 66, 59, 435, 755, 874, 1076, 558, 730, 501, 872, 1524, 580, 1124, 265, 362, 765, 618, 1050, 3502, 1798, 1801, 959, 33, 648, 533, 281, 925, 1681, 700, 1386, 156, 383, 85, 267, 163, 756, 546, 259, 313, 555, 318, 2031, 327, 415, 394, 80, 37, 770, 694, 623, 2044, 1437, 545, 1302, 46, 518, 2118, 102, 351, 3855, 658, 447, 451, 80, 449, 791, 1181, 1518, 428, 812, 288, 381, 430, 1059, 1289]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "sr = pd.Series(corp_size)\r\n",
    "print('All tokens=', sr.sum())\r\n",
    "print('Size=', sr.count())\r\n",
    "print('Mean=', sr.mean())\r\n",
    "print('Median=', sr.median())\r\n",
    "print('Moda=', sr[sr.mode()].values)\r\n",
    "print('Min=', sr.min())\r\n",
    "print('Max=', sr.max())\r\n",
    "print('Std=', sr.std())\r\n",
    "# sr.plot()\r\n",
    "print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "All tokens= 231687\n",
      "Size= 360\n",
      "Mean= 643.575\n",
      "Median= 399.5\n",
      "Moda= [474 364 178  25 501  85]\n",
      "Min= 21\n",
      "Max= 4687\n",
      "Std= 706.706082749783\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "sr.plot.hist(bins=36, figsize=(6,4))\r\n",
    "plt.savefig('test.jpg')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(corpus[0][:50])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NLTK"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk, \r\n",
    "import re\r\n",
    "import pprint\r\n",
    "from nltk import word_tokenize, FreqDist, bigrams\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nltk.download('averaged_perceptron_tagger_ru')\r\n",
    "nltk.download('rte')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text1 = re.sub(\"[\\n]\", ' ', text1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokens = word_tokenize(text1, language='russian')\r\n",
    "sents = nltk.sent_tokenize(text1, language='russian')\r\n",
    "\r\n",
    "sents_tokens = [word_tokenize(sent) for sent in sents]\r\n",
    "\r\n",
    "print(len(sents))\r\n",
    "print(sents_tokens)\r\n",
    "# pprint.pprint(sents_tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text simple analyze"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text = nltk.Text(tokens=tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(text.collocations())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(list(bigrams((text))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    " fdist1 = FreqDist(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(fdist1.most_common(30))\r\n",
    "fdist1.plot(50, cumulative=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## POS TAGS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sent_tags = nltk.pos_tag_sents(sents_tokens, lang='rus')\r\n",
    "pprint.pprint(sent_tags)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tags = nltk.pos_tag(tokens, lang='rus')\r\n",
    "print(tags)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RTE extractor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def rte_features(rtepair):\r\n",
    "    extractor = nltk.RTEFeatureExtractor(rtepair)\r\n",
    "    features = {}\r\n",
    "    features['word_overlap'] = len(extractor.overlap('word'))\r\n",
    "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\r\n",
    "    features['ne_overlap'] = len(extractor.overlap('ne'))\r\n",
    "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\r\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\r\n",
    "extractor = nltk.RTEFeatureExtractor(rtepair)\r\n",
    "print(extractor.overlap(toktype='ne'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confusion matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.corpus import brown\r\n",
    "t2 = nltk.UnigramTagger(brown.tagged_sents(categories='editorial'))\r\n",
    "\r\n",
    "def tag_list(tagged_sents):\r\n",
    "    return [tag for sent in tagged_sents for (word, tag) in sent]\r\n",
    "\r\n",
    "def apply_tagger(tagger, corpus):\r\n",
    "    return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]\r\n",
    "\r\n",
    "gold = tag_list(brown.tagged_sents(categories='editorial'))\r\n",
    "test = tag_list(apply_tagger(t2, brown.tagged_sents(categories='editorial')))\r\n",
    "cm = nltk.ConfusionMatrix(gold, test)\r\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nltk.downloader.download_gui()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsing With Context Free Grammar"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Для русского не найдены граммеры\r\n",
    "grammar = nltk.parse_cfg(\"\"\"\r\n",
    "  S -> NP VP\r\n",
    "  VP -> V NP | V NP PP\r\n",
    "  PP -> P NP\r\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\r\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\r\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\r\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\r\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\r\n",
    "  \"\"\")\r\n",
    "rd_parser = nltk.RecursiveDescentParser(TestGrammar)\r\n",
    "for tree in rd_parser.parse(sents_tokens[0]):\r\n",
    "    print(tree)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gensim"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.models import doc2vec\r\n",
    "from gensim import similarities\r\n",
    "from gensim import corpora\r\n",
    "import random\r\n",
    "from copy import copy\r\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dictionary = corpora.Dictionary(corpus)\r\n",
    "dictionary.save('./dataset/tails.dict')\r\n",
    "print(dictionary.num_docs)\r\n",
    "print(dictionary.num_nnz)\r\n",
    "print(dictionary.num_pos)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dictionary.filter_extremes()\r\n",
    "print(dictionary.num_docs)\r\n",
    "print(dictionary.num_nnz)\r\n",
    "print(dictionary.num_pos)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BoW, TfIdf, LSI"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def most_simillar(model, target, corpus, documents):    \r\n",
    "    vec_bow = dictionary.doc2bow(target)\r\n",
    "    vec = model[vec_bow]\r\n",
    "    # print(vec_lsi)\r\n",
    "\r\n",
    "    index = similarities.MatrixSimilarity(model[corpus])\r\n",
    "    sims = index[vec]  # perform a similarity query against the corpus\r\n",
    "    # print(list(enumerate(sims)))\r\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\r\n",
    "    print(\"new vector:\", target)\r\n",
    "    print()\r\n",
    "    for doc_position, doc_score in sims[:1]:\r\n",
    "        print(doc_score, documents[doc_position])\r\n",
    "    return ' '.join(documents[doc_position])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# порядок слов не учитывается в BoW\r\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in corpus]\r\n",
    "corpora.MmCorpus.serialize('./dataset/tails.mm', bow_corpus)  # store to disk, for later use"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_lsi = gensim.models.LsiModel(bow_corpus, id2word=dictionary, num_topics=300)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#doc = preproccess_text(\"Царь лежал на диване и потягивал смузи. В дом зашел Кащей и начал разговор.\")\r\n",
    "doc = corpus[0][:20]\r\n",
    "doc_pred = most_simillar(model=model_lsi, target=doc, corpus=bow_corpus, documents=corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_tfidf = gensim.models.TfidfModel(bow_corpus, normalize=True)\r\n",
    "tf_corpus = model_tfidf[bow_corpus]\r\n",
    "model_lsi_tf = gensim.models.LsiModel(corpus=tf_corpus, num_topics=300, id2word=dictionary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# doc = \"Царь лежал на диване и потягивал смузи. В дом зашел Кащей и начал разговор.\".lower().split()\r\n",
    "\r\n",
    "doc = corpus[0].copy()\r\n",
    "doc = random.choices(doc, k=30)\r\n",
    "\r\n",
    "doc_pred = most_simillar(model=model_lsi_tf, target=doc, corpus=tf_corpus, documents=corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# doc = \"Царь лежал на диване и потягивал смузи. В дом зашел Кащей и начал разговор.\".lower().split()\r\n",
    "doc = corpus[0][:20]\r\n",
    "doc_pred = most_simillar(model=model_tfidf, target=doc, corpus=tf_corpus, documents=corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word2Vec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately, the model is unable to infer vectors for unfamiliar words. This is one limitation of Word2Vec: if this limitation matters to you, check out the FastText model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = gensim.models.Word2Vec(epochs=20, vector_size=100, window=5, min_count=1)\r\n",
    "model.build_vocab(corpus)\r\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyemd import emd\r\n",
    "word_vectors = model.wv\r\n",
    "sentence1 = corpus[0][:10]\r\n",
    "sentence2 = corpus[1][:20]\r\n",
    "print(\"sentence1=\", sentence1)\r\n",
    "print(\"sentence2=\", sentence2)\r\n",
    "similarity = word_vectors.wmdistance(sentence1, sentence2)\r\n",
    "print(f\"wmdistance={similarity:.4f}\")\r\n",
    "\r\n",
    "similarity = word_vectors.n_similarity(sentence1, sentence2)\r\n",
    "print(f\"n_similarity={similarity:.4f}\")\r\n",
    "0.7067\r\n",
    "\r\n",
    "distance = word_vectors.distance('царь', 'кощей')\r\n",
    "print(f\"'царь - кощей' distance={distance:.1f}\")\r\n",
    "\r\n",
    "vector = word_vectors.get_vector('кощей', norm=True)\r\n",
    "print('кощей', vector.shape)\r\n",
    "# (100,)\r\n",
    "\r\n",
    "print(f'\"баба старик ведро\" doesnt_match={word_vectors.doesnt_match(\"баба старик ведро\".split())}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.predict_output_word(['баба'], topn=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for index, word in enumerate(model.wv.index_to_key):\r\n",
    "    if index == 10:\r\n",
    "        break\r\n",
    "    print(f\"word #{index}/{len(model.wv.index_to_key)} is {word}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\r\n",
    "from sklearn.manifold import TSNE                   # final reduction\r\n",
    "import numpy as np                                  # array handling\r\n",
    "\r\n",
    "\r\n",
    "def reduce_dimensions(model):\r\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\r\n",
    "\r\n",
    "    # extract the words & their vectors, as numpy arrays\r\n",
    "    vectors = np.asarray(model.wv.vectors)\r\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\r\n",
    "\r\n",
    "    # reduce using t-SNE\r\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\r\n",
    "    vectors = tsne.fit_transform(vectors)\r\n",
    "\r\n",
    "    x_vals = [v[0] for v in vectors]\r\n",
    "    y_vals = [v[1] for v in vectors]\r\n",
    "    return x_vals, y_vals, labels\r\n",
    "\r\n",
    "\r\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\r\n",
    "\r\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\r\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\r\n",
    "    import plotly.graph_objs as go\r\n",
    "\r\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\r\n",
    "    data = [trace]\r\n",
    "\r\n",
    "    if plot_in_notebook:\r\n",
    "        init_notebook_mode(connected=True)\r\n",
    "        iplot(data, filename='word-embedding-plot')\r\n",
    "    else:\r\n",
    "        plot(data, filename='word-embedding-plot.html')\r\n",
    "\r\n",
    "\r\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\r\n",
    "    import matplotlib.pyplot as plt\r\n",
    "    import random\r\n",
    "\r\n",
    "    random.seed(0)\r\n",
    "\r\n",
    "    plt.figure(figsize=(12, 12))\r\n",
    "    plt.scatter(x_vals, y_vals)\r\n",
    "\r\n",
    "    #\r\n",
    "    # Label randomly subsampled 25 data points\r\n",
    "    #\r\n",
    "    indices = list(range(len(labels)))\r\n",
    "    selected_indices = random.sample(indices, 25)\r\n",
    "    for i in selected_indices:\r\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\r\n",
    "\r\n",
    "try:\r\n",
    "    get_ipython()\r\n",
    "except Exception:\r\n",
    "    plot_function = plot_with_matplotlib\r\n",
    "else:\r\n",
    "    plot_function = plot_with_plotly\r\n",
    "\r\n",
    "plot_function(x_vals, y_vals, labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### W2V Phrases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.models import Phrases\r\n",
    "\r\n",
    "bigram_transformer = Phrases(corpus)\r\n",
    "\r\n",
    "model_bigram = gensim.models.Word2Vec(bigram_transformer[corpus], min_count=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x_vals, y_vals, labels = reduce_dimensions(model_bigram)\r\n",
    "plot_function(x_vals, y_vals, labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ruscorpora word2vec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ruscorpora_vectors = gensim.downloader.load('word2vec-ruscorpora-300')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list(ruscorpora_vectors.key_to_index.keys())[:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import zipfile\r\n",
    "import wget\r\n",
    "\r\n",
    "model_path = 'c:/models/rusvector/'\r\n",
    "model_url = 'http://vectors.nlpl.eu/repository/11/180.zip'\r\n",
    "m = wget.download(model_url, out=model_path)\r\n",
    "model_file = model_path + model_url.split('/')[-1]\r\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\r\n",
    "    stream = archive.open('model.bin')\r\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "list(model.key_to_index.keys())[:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# надо суффикс проставлять для слова с частеречием\r\n",
    "vec1 = model.most_similar(positive=['один_NUM'])\r\n",
    "print(vec1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Doc2Vec"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import logging\r\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data = corpus[:round(len(corpus) * 0.8)]\r\n",
    "test_data = corpus[round(len(corpus) * 0.8):]\r\n",
    "print(len(train_data), len(test_data))\r\n",
    "\r\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(tokens, [i]) for i, tokens in enumerate(train_data)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_corpus[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_d2v = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=1, epochs=40)\r\n",
    "model_d2v.build_vocab(train_corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_d2v.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vector = model_d2v.infer_vector(corpus[0][:10])\r\n",
    "print(vector)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ranks = []\r\n",
    "second_ranks = []\r\n",
    "for doc_id in range(len(train_corpus)):\r\n",
    "    inferred_vector = model_d2v.infer_vector(train_corpus[doc_id].words)\r\n",
    "    sims = model_d2v.dv.most_similar([inferred_vector], topn=len(model_d2v.dv))\r\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\r\n",
    "    ranks.append(rank)\r\n",
    "\r\n",
    "    second_ranks.append(sims[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import collections\r\n",
    "\r\n",
    "counter = collections.Counter(ranks)\r\n",
    "print(counter)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc_id = random.choice(range(len(train_corpus)))\r\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\r\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\r\n",
    "inferred_vector = model_d2v.infer_vector(train_corpus[doc_id].words)\r\n",
    "sims = model_d2v.dv.most_similar([inferred_vector], topn=len(model_d2v.dv))\r\n",
    "for label, index in sims[:3]:\r\n",
    "    print(f'{label} {index:2.2%}: {\" \".join(train_corpus[label].words)}')\r\n",
    "    print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### !! убрать надо ударения в словах"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc_id = random.randint(0, len(test_data) - 1)\r\n",
    "inferred_vector = model_d2v.infer_vector(test_data[doc_id])\r\n",
    "sims = model_d2v.dv.most_similar([inferred_vector], topn=len(model_d2v.dv))\r\n",
    "\r\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_data[doc_id])))\r\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model_d2v)\r\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\r\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FastText"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Особенности:\r\n",
    "\r\n",
    "* фастекст определяет векторы подстрок входящих в слово и выдает суммарный вектор на их основе. \r\n",
    "* В отличии от w2v может обрабатывать слова не из словаря если хотя бы одна из подстрок этого слова (к примеру корень) есть в векторном пространстве"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_ft = gensim.models.FastText(vector_size=100)\r\n",
    "\r\n",
    "# build the vocabulary\r\n",
    "model_ft.build_vocab(corpus)\r\n",
    "\r\n",
    "# train the model\r\n",
    "model_ft.train(\r\n",
    "    corpus, \r\n",
    "    epochs=model_ft.epochs,\r\n",
    "    total_examples=model_ft.corpus_count, \r\n",
    "    total_words=model_ft.corpus_total_words,\r\n",
    ")\r\n",
    "\r\n",
    "print(model_ft)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wv = model_ft.wv\r\n",
    "print(wv)\r\n",
    "\r\n",
    "print('путин' in wv.key_to_index)\r\n",
    "try:\r\n",
    "    word = 'путин'\r\n",
    "    print(wv[word])\r\n",
    "\r\n",
    "except KeyError as err:\r\n",
    "    print(err)\r\n",
    "\r\n",
    "print(wv.similarity(\"путь\", \"путин\"))\r\n",
    "print(wv.most_similar(\"путь\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sentence1 = corpus[0][:10]\r\n",
    "sentence2 = corpus[10][:20]\r\n",
    "print(wv.n_similarity(sentence1, sentence2))\r\n",
    "distance = wv.wmdistance(sentence1, sentence2)\r\n",
    "print(f\"Word Movers Distance is {distance} (lower means closer)\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Movers Distance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Word Movers Distance on fastText')\r\n",
    "sentence1 = corpus[0][:10]\r\n",
    "sentence2 = preproccess_text('сегодня в новостях опять кого то убили ни за что, а ведь он мог учиться в университете')\r\n",
    "print('sentence1', sentence1)\r\n",
    "print('sentence2', sentence2)\r\n",
    "distance = wv.wmdistance(sentence1, sentence2)\r\n",
    "print(f\"Word Movers Distance is {distance} (lower means closer)\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word_vectors_d2v = model_d2v.wv\r\n",
    "print('Word Movers Distance on Doc2Vec')\r\n",
    "sentence1 = corpus[0][:10]\r\n",
    "# sentence2 = preproccess_text('сегодня в новостях опять кого то убили ни за что, а ведь он мог учиться в университете')\r\n",
    "sentence2 = corpus[0][5:15]\r\n",
    "print('sentence1', sentence1)\r\n",
    "print('sentence2', sentence2)\r\n",
    "distance = word_vectors_d2v.wmdistance(sentence1, sentence2)\r\n",
    "print(f\"Word Movers Distance is {distance} (lower means closer)\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Soft Cosine Measure"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.similarities import SparseTermSimilarityMatrix, WordEmbeddingSimilarityIndex, SoftCosineSimilarity"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Word Movers Distance on Doc2Vec')\r\n",
    "sentence1 = corpus[0][:20]\r\n",
    "sentence2 = corpus[0][20:40]\r\n",
    "sentence3 = preproccess_text('сегодня в новостях опять кого то убили ни за что, а ведь он мог учиться в университете')\r\n",
    "print('sentence1', sentence1)\r\n",
    "print('sentence2', sentence2)\r\n",
    "print('sentence3', sentence3)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = gensim.models.Word2Vec(epochs=20, vector_size=100, window=5, min_count=1)\r\n",
    "model.build_vocab(corpus)\r\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "termsim_index = WordEmbeddingSimilarityIndex(model.wv)\r\n",
    "dictionary = gensim.corpora.Dictionary(corpus)\r\n",
    "bow_corpus = [dictionary.doc2bow(document) for document in corpus]\r\n",
    "similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sent_1 = dictionary.doc2bow(sentence1)\r\n",
    "sent_2 = dictionary.doc2bow(sentence2)\r\n",
    "sent_3 = dictionary.doc2bow(sentence3)\r\n",
    "print(sentence1, '\\n', sentence2)\r\n",
    "similarity = similarity_matrix.inner_product(sent_1, sent_2, normalized=(True, True))\r\n",
    "print('similarity = %.4f' % similarity)\r\n",
    "print()\r\n",
    "print(sentence1, '\\n', sentence3)\r\n",
    "similarity = similarity_matrix.inner_product(sent_1, sent_3, normalized=(True, True))\r\n",
    "print('similarity = %.4f' % similarity)\r\n",
    "print()\r\n",
    "print(sentence1, '\\n', sentence1)\r\n",
    "similarity = similarity_matrix.inner_product(sent_1, sent_1, normalized=(True, True))\r\n",
    "print('similarity = %.4f' % similarity)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pickle\r\n",
    "import zipfile\r\n",
    "import wget\r\n",
    "from preproccess_text import tag_ud"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "сorpus_file = \"./dataset/corpus.txt\"\r\n",
    "# with open(corpus_file, \"wb\") as f:\r\n",
    "#     pickle.dump(corpus, f)\r\n",
    "\r\n",
    "with open(сorpus_file, \"rb\") as f:   # Unpickling\r\n",
    "    corpus = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "corpus = []\r\n",
    "stops = set(stopwords.words('russian'))\r\n",
    "root_path = 'C:\\\\Users\\\\edbon\\\\devproj\\\\faiky-tails\\\\dataset\\\\raw\\\\'\r\n",
    "modelfile = './udpipe_syntagrus.model'\r\n",
    "with open(root_path + '001 Арысь - поле.txt', 'r', encoding='utf-8') as f:\r\n",
    "    doc = tag_ud(text=f.read(), modelfile=modelfile, stop_words=stops, keep_pos=True, keep_punct=False)\r\n",
    "corpus.append(doc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model_path = 'c:/models/rusvector/'\r\n",
    "model_file = model_path + '180.zip'\r\n",
    "if not os.path.exists(model_file):\r\n",
    "    model_url = 'http://vectors.nlpl.eu/repository/11/180.zip'\r\n",
    "    m = wget.download(model_url, out=model_path)\r\n",
    "    model_file = model_path + model_url.split('/')[-1]\r\n",
    "\r\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\r\n",
    "    stream = archive.open('model.bin')\r\n",
    "    model_rv = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print(stops)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'почти', 'над', 'тогда', 'с', 'при', 'вдруг', 'теперь', 'без', 'после', 'про', 'сейчас', 'как', 'по', 'кто', 'есть', 'тем', 'вот', 'что', 'мой', 'какая', 'она', 'тут', 'ведь', 'нет', 'сам', 'всегда', 'всю', 'впрочем', 'перед', 'потому', 'зачем', 'свою', 'и', 'а', 'так', 'чтобы', 'не', 'да', 'была', 'тебя', 'ему', 'вам', 'может', 'конечно', 'вы', 'со', 'из', 'то', 'него', 'чем', 'иногда', 'более', 'мне', 'этого', 'на', 'один', 'ее', 'разве', 'того', 'два', 'эту', 'об', 'нее', 'всех', 'там', 'под', 'будет', 'были', 'его', 'все', 'ли', 'ничего', 'они', 'уж', 'чтоб', 'опять', 'меня', 'эти', 'всего', 'куда', 'моя', 'лучше', 'много', 'у', 'вас', 'для', 'мы', 'или', 'ней', 'во', 'чего', 'надо', 'какой', 'уже', 'чуть', 'от', 'хорошо', 'еще', 'совсем', 'был', 'никогда', 'тот', 'он', 'можно', 'ни', 'этой', 'ну', 'нас', 'о', 'этом', 'до', 'себя', 'через', 'раз', 'быть', 'больше', 'ж', 'за', 'бы', 'но', 'потом', 'ты', 'хоть', 'между', 'этот', 'даже', 'когда', 'будто', 'такой', 'нибудь', 'том', 'наконец', 'им', 'них', 'я', 'где', 'тоже', 'другой', 'если', 'ей', 'в', 'ним', 'только', 'их', 'себе', 'было', 'три', 'здесь', 'к', 'же', 'нельзя'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# надо суффикс проставлять для слова с частеречием\r\n",
    "\r\n",
    "bad_words = set()\r\n",
    "for word in corpus[0]:\r\n",
    "    try:\r\n",
    "        model_rv[word]\r\n",
    "    except KeyError as err:\r\n",
    "        bad_words.add(word)\r\n",
    "print(bad_words)\r\n",
    "\r\n",
    "for word in bad_words:\r\n",
    "    w = word.split('_')[0]\r\n",
    "    \r\n",
    "    if w in stops:\r\n",
    "        print(word)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'арысь_NOUN', 'весь_DET', 'невзлюбить_PROPN', 'стариковый_ADJ', 'свой_DET', 'падчерицыный_ADV', 'вместо_ADP', 'никто_PRON'}\n",
      "арысь\n",
      "весь\n",
      "невзлюбить\n",
      "стариковый\n",
      "свой\n",
      "падчерицыный\n",
      "вместо\n",
      "никто\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('tf': conda)"
  },
  "interpreter": {
   "hash": "2d3a13fa35461c6d7f441fbb3d93fc4f14860aa527e8f914775d5e57b8364c02"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}