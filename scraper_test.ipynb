{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import re\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "url = 'http://hyaenidae.narod.ru/'\r\n",
    "pages_root_prefix = 'story'\r\n",
    "pages_root_count = 5\r\n",
    "ds_path = 'dataset/raw/'\r\n",
    "descr_file_path = 'dataset/description.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "if os.path.exists(descr_file_path):\r\n",
    "    os.remove(descr_file_path)\r\n",
    "descr_file = open(descr_file_path, 'a', encoding='utf-8')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "articles = []\r\n",
    "for i in range(1, pages_root_count + 1):\r\n",
    "    try:\r\n",
    "        r_url = url + pages_root_prefix + str(i)\r\n",
    "        r = requests.get(r_url, headers={'Accept-Language': 'ru-RU,ru;q=0.5'})\r\n",
    "    except Exception:\r\n",
    "        print(f'Page {r_url} connection errors!')\r\n",
    "        print(r.reason)\r\n",
    "        break\r\n",
    "    else:\r\n",
    "        if r:\r\n",
    "            payload = BeautifulSoup(r.content, features='html.parser')\r\n",
    "            \r\n",
    "            \r\n",
    "            articles_left = payload.find_all(class_='rsleft')\r\n",
    "            if articles_left:\r\n",
    "                for tag in articles_left[0].children:\r\n",
    "                    if tag.name == 'a':\r\n",
    "                        article_title = tag.string.rstrip(\";\")\r\n",
    "                        text = scrape_text(tag[\"href\"])\r\n",
    "                        if not text:                        \r\n",
    "                            print(f'{article_title}, {tag[\"href\"]}, {len(text)}')\r\n",
    "                        articles.append((article_title, tag['href'], text))\r\n",
    "                        write_text(article_title, text)\r\n",
    "                        descr_file.write(f'{article_title}|{tag[\"href\"]}\\n')\r\n",
    "            \r\n",
    "            articles_right = payload.find_all(class_='rsright')\r\n",
    "            if articles_right:\r\n",
    "                for tag in articles_right[0].children:\r\n",
    "                    if tag.name == 'a':\r\n",
    "                        article_title = tag.string.rstrip(\";\")\r\n",
    "                        text = scrape_text(tag[\"href\"])\r\n",
    "                        if not text:                        \r\n",
    "                            print(f'{article_title}, {tag[\"href\"]}, {len(text)}')\r\n",
    "                        articles.append((article_title, tag['href'], text))\r\n",
    "                        write_text(article_title, text)\r\n",
    "                        descr_file.write(f'{article_title}|{tag[\"href\"]}\\n')\r\n",
    "\r\n",
    "            articles_liupp = payload.find_all(class_='liupp')\r\n",
    "            if articles_liupp:\r\n",
    "                for part in articles_liupp:\r\n",
    "                    for tag in part.children:\r\n",
    "                        if tag.name == 'a':\r\n",
    "                            article_title = tag.string.rstrip(\";\")\r\n",
    "                            text = scrape_text(tag[\"href\"])\r\n",
    "                            if not text:\r\n",
    "                                print(f'{article_title}, {tag[\"href\"]}, {len(text)}')\r\n",
    "                            articles.append((article_title, tag['href'], text))\r\n",
    "                            write_text(article_title, text)\r\n",
    "                            descr_file.write(f'{article_title}|{tag[\"href\"]}\\n')\r\n",
    "\r\n",
    "            articles_all = payload.find_all(class_='all')\r\n",
    "            if articles_all:\r\n",
    "                for tag in articles_all:\r\n",
    "                    article_title = tag.string.rstrip(\";\")\r\n",
    "                    text = scrape_text(tag[\"href\"])\r\n",
    "                    if not text:                    \r\n",
    "                        print(f'{article_title}, {tag[\"href\"]}, {len(text)}')\r\n",
    "                    articles.append((article_title, tag['href'], text))\r\n",
    "                    write_text(article_title, text)\r\n",
    "                    descr_file.write(f'{article_title}|{tag[\"href\"]}\\n')\r\n",
    "\r\n",
    "            start_count = 270\r\n",
    "            articles_sk = payload.find_all(class_='sk')\r\n",
    "            if articles_sk:\r\n",
    "                for part in articles_sk:                \r\n",
    "                    for tag in part.children:\r\n",
    "                        if tag.name == 'a':                            \r\n",
    "                            article_title = str(start_count) + '. ' + tag.string.rstrip(';')\r\n",
    "                            text = scrape_text(tag[\"href\"])\r\n",
    "                            if not text:\r\n",
    "                                print(f'{article_title}, {tag[\"href\"]}, {len(text)}')\r\n",
    "                            articles.append((article_title, tag['href'], text))\r\n",
    "                            write_text(article_title, text)\r\n",
    "                            descr_file.write(f'{article_title}|{tag[\"href\"]}\\n')\r\n",
    "                            start_count += 1            \r\n",
    "        else:\r\n",
    "            print(f\"The URL returned {r.status_code}!\")\r\n",
    "print(f\"Всего сказок: {len(articles)}, последний порядковый номер документа: {articles[-1][0].split('.')[0]}\")\r\n",
    "print(f'Кол-во сохренных файлов: {len(os.listdir(ds_path))}')\r\n",
    "\r\n",
    "descr_file.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Всего сказок: 360, последний порядковый номер документа: 361\n",
      "Кол-во сохренных файлов: 360\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def scrape_text(url: str) -> str:\r\n",
    "    try:\r\n",
    "        req = requests.get(url, headers={'Accept-Language': 'ru-RU,ru;q=0.5'})        \r\n",
    "    except Exception:\r\n",
    "        print(f'Page {r_url} connection errors!')\r\n",
    "        print(r.reason)\r\n",
    "    if req:\r\n",
    "        payload = BeautifulSoup(req.content, features='html.parser')         \r\n",
    "        \r\n",
    "        text = payload.find_all(id='AR')\r\n",
    "        if text:\r\n",
    "            return text[0].text\r\n",
    "        \r\n",
    "        text = payload.find_all(id='PAR')\r\n",
    "        if text:\r\n",
    "            return text[0].text\r\n",
    "        \r\n",
    "        print(f'Find nothing on {url}')\r\n",
    "        return \"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "\r\n",
    "def write_text(file_name: str, text: str):\r\n",
    "    file_name = re.sub('[?.]', '', file_name)\r\n",
    "    path = ds_path + file_name + '.txt'\r\n",
    "    with open(path, 'w', encoding='utf-8') as f:\r\n",
    "        f.write(text)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('tf': conda)"
  },
  "interpreter": {
   "hash": "2d3a13fa35461c6d7f441fbb3d93fc4f14860aa527e8f914775d5e57b8364c02"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}