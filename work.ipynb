{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.model.data_loader import ParagraphDataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "from src.model.loss import ParagraphLoss\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from src.model.logger import Logger\n",
    "import os\n",
    "import pandas as pd\n",
    "import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParagraphDataset(Dataset):\n",
    "    def __init__(self, data_file, encoder, max_size=None, n_ctx=102, n_gen=401, include_neigh=False,\n",
    "                 include_discourse_type=True, include_kw=True, dim=0 ,debug_mode=False):\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            self.data = f.readlines()\n",
    "\n",
    "        if include_neigh:\n",
    "            self.prev = []\n",
    "            fn = \".\".join(data_file.split(\".\")[:-1]) + \"_gpt2.pkl\"\n",
    "            if debug_mode:\n",
    "                fn = \".\".join(data_file.split(\".\")[:-1]) + \"_gpt.pkl\"\n",
    "            with open(fn, 'rb') as fp:\n",
    "                for k in range(len(self.data)):\n",
    "                    temp = pickle.load(fp)\n",
    "                    assert temp[0] == k and temp[1] == self.data[k].decode('utf-8', 'ignore').split(\"\\t\")[-1].replace(\n",
    "                        \"<o>\", \"\").strip()\n",
    "                    self.prev.append(temp[2])\n",
    "        else:\n",
    "            self.prev = None\n",
    "\n",
    "        self.dids = []\n",
    "        for d in range(1, len(self.data)):\n",
    "            t = self.data[d].decode(\"utf-8\", \"ignore\").strip().split('\\t')\n",
    "            if len(t) == 7 and t[5].replace(\"<o>\", \"\").strip() != \"\":\n",
    "                try:\n",
    "                    x, y = int(t[0].split(\"_\")[-1]), int(t[4])\n",
    "                    self.dids.append(d)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        if max_size is not None:\n",
    "            self.dids = self.dids[:max_size]\n",
    "        self.encoder = encoder\n",
    "        self.ctx = n_ctx - 2\n",
    "        self.gen = n_gen - 1\n",
    "        self.dim = dim\n",
    "        self.len = len(self.data)\n",
    "        self.include_neigh = include_neigh\n",
    "        self.include_discourse_type = include_discourse_type\n",
    "        self.include_kw = include_kw\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx = self.dids[index]\n",
    "        csv_data = self.data[idx].decode(\"utf-8\", \"ignore\").strip().split('\\t')\n",
    "        kws = csv_data[2].split(\"[SEP]\")\n",
    "        tgt_phrase = self.encoder.encode(csv_data[5].replace(\"<o>\", \"\"),  add_special_tokens=False)[:self.gen] # add_prefix_space=True,\n",
    "        start = torch.LongTensor([self.encoder.bos_token_id])\n",
    "        clstok = torch.LongTensor([self.encoder.cls_token_id])\n",
    "        end = torch.LongTensor([self.encoder.eos_token_id])\n",
    "        tstart = torch.LongTensor([self.encoder.convert_tokens_to_ids('_t_')])\n",
    "        istart = torch.LongTensor([self.encoder.convert_tokens_to_ids('_i_')])\n",
    "        bstart = torch.LongTensor([self.encoder.convert_tokens_to_ids('_b_')])\n",
    "        cstart = torch.LongTensor([self.encoder.convert_tokens_to_ids('_c_')])\n",
    "        keytok = torch.LongTensor([self.encoder.convert_tokens_to_ids('_kw_')])\n",
    "        endkeytok = torch.LongTensor([self.encoder.convert_tokens_to_ids('_endkw_')])\n",
    "        \n",
    "        if self.include_discourse_type:\n",
    "            starttyptok = bstart\n",
    "            if int(csv_data[0].split(\"_\")[-1]) == 0:\n",
    "                starttyptok = istart\n",
    "            elif int(csv_data[0].split(\"_\")[-1]) == int(csv_data[4]) - 1:\n",
    "                starttyptok = cstart\n",
    "        else:\n",
    "            starttyptok = clstok\n",
    "\n",
    "        print(starttyptok)\n",
    "        \n",
    "        pad_output = torch.zeros(self.ctx + self.gen + 3).long()\n",
    "        mask_output = torch.zeros(self.ctx + self.gen + 3).long()\n",
    "\n",
    "        pad_output[0] = start\n",
    "        if self.include_kw:\n",
    "            i = 1\n",
    "            for k in kws:\n",
    "                if i - 1 >= self.ctx:\n",
    "                    break\n",
    "                enck = self.encoder.encode(k.strip(),  add_special_tokens=False)[:self.ctx - i] # add_prefix_space=True,\n",
    "                # print(enck, i)\n",
    "                pad_output[i:i + len(enck)] = torch.LongTensor(enck)\n",
    "                pad_output[i + len(enck)] = keytok\n",
    "                i += len(enck) + 1\n",
    "            pad_output[i - 1] = endkeytok\n",
    "            mask_output[0:i] = torch.ones(i).long()\n",
    "\n",
    "        pad_output[self.ctx + 1] = starttyptok if self.include_discourse_type else clstok  # [101] -> discourse tag\n",
    "        pad_output[self.ctx + 1 + 1:self.ctx + 1 + 1 + len(tgt_phrase)] = torch.LongTensor(tgt_phrase)\n",
    "        pad_output[self.ctx + 1 + 1 + len(tgt_phrase)] = end\n",
    "\n",
    "        # Mask\n",
    "        mask_output[self.ctx + 1:self.ctx + 1 + len(tgt_phrase) + 2] = torch.ones(len(tgt_phrase) + 2).long()\n",
    "\n",
    "        if self.include_neigh:\n",
    "            n = torch.FloatTensor(self.prev[idx].flatten())\n",
    "        else:\n",
    "            n = torch.zeros(self.dim, dtype=torch.float64)\n",
    "        return pad_output, mask_output, n\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = GPT2Tokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\", add_prefix_space=False)\n",
    "\n",
    "encoder.add_special_tokens({'bos_token':'_start_',\n",
    "                                     'cls_token':'_classify_',\n",
    "                                     'eos_token':'_end_',\n",
    "                                     'additional_special_tokens': ['_kw_','_endkw_', '_t_', '_i_', '_b_', '_c_']\n",
    "                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"include_kw\": True,\n",
    "    \"include_discourse_type\": True,\n",
    "    \"max_size\": 10,\n",
    "    \"n_ctx\": 102,\n",
    "    \"gen_len\": 400,\n",
    "    \"include_neigh\": False,\n",
    "    \"dim\": 768,\n",
    "    \"lr\": 6.25e-5,\n",
    "    \"b1\": 0.9,\n",
    "    \"b2\": 0.999,\n",
    "    \"e\": 1e-8,\n",
    "    \"num_epochs\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ParagraphDataset('dataset/plot/train_encoded.csv', encoder, max_size=config[\"max_size\"], n_ctx=config[\"n_ctx\"], n_gen=config[\"gen_len\"],\n",
    "                               include_neigh=config[\"include_neigh\"], include_discourse_type=config[\"include_discourse_type\"], \n",
    "                               include_kw=config[\"include_kw\"], dim=config[\"dim\"])\n",
    "\n",
    "test_dataset = ParagraphDataset('dataset/plot/val_encoded.csv', encoder, max_size=config[\"max_size\"], n_ctx=config[\"n_ctx\"], n_gen=config[\"gen_len\"],\n",
    "                               include_neigh=config[\"include_neigh\"], include_discourse_type=config[\"include_discourse_type\"], \n",
    "                               include_kw=config[\"include_kw\"], dim=config[\"dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "    # print(batch[0][0].size())\n",
    "    print(encoder.decode(batch[0][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(model, args, device, compute_loss_fct):\n",
    "    for arg in args:\n",
    "        if arg is not None:\n",
    "            arg = arg.to(device)\n",
    "\n",
    "    output = model(*args)\n",
    "    \n",
    "    args[0] = args[0].to(device)\n",
    "    args[1] = args[1].to(device)\n",
    "    \n",
    "    allloss = compute_loss_fct(output, args[0], args[1])\n",
    "    \n",
    "    return allloss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(bestloss, start_iter, running_loss, model, compute_loss_fct, model_opt, train_loader, val_loader, train_log_interval, val_log_interval, device, beam, gen_len, k,p, decoding_strategy, accum_iter, desc_str, save_dir, logger, text_encoder, show_progress=False, summary_loss=None, my_local_dir='checkpoints_local'):\n",
    "    '''\n",
    "    Run a single epoch, log results, and save best checkpoint\n",
    "    '''\n",
    "    if show_progress:\n",
    "        train_bar = tqdm(iterable=train_loader, desc=desc_str)\n",
    "    else:\n",
    "        train_bar = train_loader\n",
    "\n",
    "    for i, batchargs in enumerate(train_bar, start_iter):\n",
    "        \n",
    "        num_updates = i // accum_iter\n",
    "        model.train()\n",
    "        loss = run_batch(model, batchargs, device, compute_loss_fct)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        running_loss += float(loss.detach().item())\n",
    "        if show_progress:\n",
    "            train_bar.set_postfix(loss=running_loss / ((train_log_interval * accum_iter) if num_updates % train_log_interval == 0 and num_updates != 0 else i % (train_log_interval * accum_iter)))\n",
    "\n",
    "        if i % accum_iter == 0:\n",
    "            model_opt.step()\n",
    "            model_opt.zero_grad()\n",
    "            torch.cuda.empty_cache()\n",
    "        if num_updates % train_log_interval == 0 and i % accum_iter == 0:\n",
    "            logger.scalar_summary(\"Training\", num=running_loss, denom=(train_log_interval * accum_iter), step=num_updates)\n",
    "            print(\"training loss %.2f\" % (running_loss/float(train_log_interval * accum_iter)))\n",
    "            running_loss = 0\n",
    "\n",
    "        # if num_updates % 1000 == 0 and i % accum_iter == 0:\n",
    "        #     val_loss, scores = evaluate(val_loader, train_log_interval, model, text_encoder, device, beam, gen_len, k, p, decoding_strategy, compute_loss_fct, min_len=args.min_len)\n",
    "\n",
    "        #     logger.scalar_summary(\"Validation\", num=val_loss, denom=len(val_loader), step=num_updates)\n",
    "        #     # if sum(val_loss) < bestloss or bestloss == -1:\n",
    "        #     lv = get_loss_value(val_loss, len(val_loader))\n",
    "        #     if (not math.isnan(lv)) and (bestloss == -1 or lv < bestloss):\n",
    "        #         bestloss = lv\n",
    "        #         save_checkpoint(i + 1, running_loss, model.state_dict(), model_opt.state_dict(), save_dir, my_local_dir)\n",
    "\n",
    "\n",
    "    # val_loss, scores = evaluate(val_loader, train_log_interval, model, text_encoder, device, beam, gen_len, k, p, decoding_strategy, compute_loss_fct, min_len=args.min_len)\n",
    "    # for key, value in scores.items():\n",
    "    #     for key2, value2 in value.items():\n",
    "    #         logger.rouge_summary(\"{}/{}\".format(key, key2), value2, num_updates)\n",
    "    # print(\"Validation rouge: \" + str(scores.items()))\n",
    "    # logger.scalar_summary(\"Validation\", num=val_loss, denom=len(val_loader), step=num_updates)\n",
    "    # lv = get_loss_value(val_loss, len(val_loader))\n",
    "    # if (not math.isnan(lv)) and (bestloss == -1 or lv < bestloss):\n",
    "    #     bestloss = lv\n",
    "    #     save_checkpoint(i + 1, running_loss, model.state_dict(), model_opt.state_dict(), save_dir, my_local_dir)\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return i + 1, running_loss, bestloss, num_updates # , lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'savedir'\n",
    "experiment_name = 'gpt3'\n",
    "print(\"Creating directories\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, experiment_name), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, experiment_name), exist_ok=True)\n",
    "\n",
    "\n",
    "save_dir = os.path.join(output_dir, experiment_name, \"checkpoints\")\n",
    "save_dir_local = \"checkpoints_local\"\n",
    "desc = \"Desc\"\n",
    "data_dir = 'dataset/plot'\n",
    "log_dir = os.path.join(output_dir, experiment_name, \"logs\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(save_dir_local, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_interval = 4\n",
    "val_log_interval = 4\n",
    "beam = 0\n",
    "p = 90\n",
    "k = 0\n",
    "decoding_strategy = 0\n",
    "accum_iter = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger = Logger(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "model_opt = AdamW(filter(lambda p : p.requires_grad, model.parameters()),\n",
    "                        lr=config['lr'],\n",
    "                        betas=(config['b1'], config['b2']),\n",
    "                        eps=config['e'])\n",
    "\n",
    "\n",
    "\n",
    "lm_loss = ParagraphLoss(criterion, n_ctx=config[\"n_ctx\"], gen_len=config[\"gen_len\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestloss = -1\n",
    "start_iter, running_loss = 1,0\n",
    "prevloss = 1000\n",
    "\n",
    "for i in range(config['num_epochs']):\n",
    "    start_iter, running_loss, bestloss, updates, val_loss1 = run_epoch(bestloss, start_iter, running_loss, model, lm_loss, model_opt, train_loader, val_loader, train_log_interval, val_log_interval, device, beam, config['gen_len'], k, p, decoding_strategy, accum_iter, \"FT Training Epoch [{}/{}]\".format(i + 1, config['num_epochs']), save_dir, logger, encoder, show_progress=True, my_local_dir='save_dir_local')\n",
    "    print(\"VAL LOSS: \", str(val_loss1))\n",
    "    if val_loss1 > prevloss or math.isnan(val_loss1):\n",
    "        break\n",
    "    prevloss = val_loss1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/plot/val_encoded.csv', sep='\\t')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \" \".join(df['[KEYWORDS]'][0].split('[SEP]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoder.encode(context, return_tensors='pt')\n",
    "print(context)\n",
    "print(input_ids.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=400, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(encoder.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50263])\n"
     ]
    }
   ],
   "source": [
    "pad_output, attention_mask, n = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_toks = pad_output[:, :config['n_ctx']] # includes delimiter\n",
    "target_toks = pad_output[:, config['n_ctx']:]\n",
    "target_toks = target_toks[:, attention_mask[:, config['n_ctx']:][0] == 1]\n",
    "\n",
    "XMB = pad_output[:, :config['n_ctx']]\n",
    "mask = attention_mask[:, :config['n_ctx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.decode(XMB[0], skip_special_tokens=True))\n",
    "print(encoder.decode(target_toks[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_output = model.generate(\n",
    "    XMB, \n",
    "    attention_mask=mask,\n",
    "    min_length = 100,\n",
    "    max_length=512, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True, \n",
    "    eos_token_id=encoder.eos_token_id,\n",
    "    num_return_sequences=2    \n",
    ")\n",
    "\n",
    "print(\"Context:\\n\" + 100 * '-')\n",
    "print(encoder.decode(XMB[0], skip_special_tokens=True))\n",
    "print()\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(encoder.decode(beam_output[:, config['n_ctx']:][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = encoder.decode(XMB[0], skip_special_tokens=True)\n",
    "hyps1 = encoder.decode(beam_output[:, config['n_ctx']:][0], skip_special_tokens=True)\n",
    "hyps2 = encoder.decode(beam_output[:, config['n_ctx']:][1], skip_special_tokens=True)\n",
    "\n",
    "print(\"Context:\\n\" + 100 * '-')\n",
    "print(refs)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(hyps1)\n",
    "print(hyps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_scores(hyps, refs):       \n",
    "    rouge_scorer = rouge.Rouge()\n",
    "    averaged_scores = rouge_scorer.get_scores(hyps, refs, avg=True)\n",
    "    return averaged_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_average_scores(hyps2, refs))\n",
    "# scores = rouge_scorer.get_scores(hyps, hyps, avg=True)\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50259 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sample_output = model.generate(\n",
    "    XMB, \n",
    "    attention_mask=mask,\n",
    "    max_length=512, \n",
    "    do_sample=True,\n",
    "    top_p=0.90, \n",
    "    top_k=0,\n",
    "    eos_token_id=encoder.eos_token_id,\n",
    "    min_length = 100,\n",
    "    # num_return_sequences=2, \n",
    "    temperature=0.7, # не используется в пломашинс\n",
    "    # no_repeat_ngram_size=2, # не используется в пломашинс\n",
    "    repetition_penalty = 1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XMB[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = encoder.decode(XMB[0], skip_special_tokens=False)\n",
    "hyps1 = encoder.decode(sample_output[:, config['n_ctx']:][0], skip_special_tokens=False)\n",
    "\n",
    "print(\"Context:\\n\" + 100 * '-')\n",
    "print(refs)\n",
    "print(\"Output1:\\n\" + 100 * '-')\n",
    "print(hyps1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = encoder.decode(XMB[0], skip_special_tokens=True)\n",
    "hyps1 = encoder.decode(sample_output[:, config['n_ctx']:][0], skip_special_tokens=True)\n",
    "hyps2 = encoder.decode(sample_output[:, config['n_ctx']:][1], skip_special_tokens=True)\n",
    "\n",
    "print(\"Context:\\n\" + 100 * '-')\n",
    "print(refs)\n",
    "print(\"Output1:\\n\" + 100 * '-')\n",
    "print(hyps1)\n",
    "print(\"Output2:\\n\" + 100 * '-')\n",
    "print(hyps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hyps1:\", get_average_scores( hyps1, refs))\n",
    "print(\"hyps2:\", get_average_scores( hyps2, refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.generate_utils import toks_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': 'vocab.json', 'merges_file': 'merges.txt'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder._convert_id_to_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.decoders import ByteLevel\n",
    "decoder = ByteLevel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257 '_start_'\n",
      "6092 ' лу'\n",
      "299 'то'\n",
      "1427 'ню'\n",
      "2356 'шка'\n",
      "50260 '_kw_'\n",
      "20480 ' уро'\n",
      "3391 'нила'\n",
      "505 ' его'\n",
      "309 ' на'\n",
      "2530 ' заг'\n",
      "1344 'нет'\n",
      "454 'ку'\n",
      "289 ' и'\n",
      "1373 ' тут'\n",
      "587 ' пре'\n",
      "5162 'вели'\n",
      "2306 'ким'\n",
      "6485 ' голосом'\n",
      "24133 ' закричала'\n",
      "289 ' и'\n",
      "2337 ' зав'\n",
      "412 'оп'\n",
      "1154 'ила'\n",
      "50260 '_kw_'\n",
      "15768 ' привяз'\n",
      "414 'али'\n",
      "745 ' они'\n",
      "282 ' в'\n",
      "4907 ' воро'\n",
      "2662 'тах'\n",
      "442 ' х'\n",
      "304 'ом'\n",
      "487 'ут'\n",
      "289 ' и'\n",
      "6204 ' пал'\n",
      "1082 'ками'\n",
      "282 ' в'\n",
      "5136 'гон'\n",
      "4283 'яют'\n",
      "282 ' в'\n",
      "1177 ' этот'\n",
      "442 ' х'\n",
      "304 'ом'\n",
      "487 'ут'\n",
      "12834 ' лошадь'\n",
      "50260 '_kw_'\n",
      "367 ' а'\n",
      "3253 ' сама'\n",
      "502 ' то'\n",
      "289 ' и'\n",
      "1743 ' дело'\n",
      "16381 ' ходит'\n",
      "281 ' с'\n",
      "4939 ' лож'\n",
      "27487 'кою'\n",
      "282 ' в'\n",
      "16921 ' погреб'\n",
      "349 ' за'\n",
      "23913 ' смет'\n",
      "23939 'аной'\n",
      "50260 '_kw_'\n",
      "24903 ' мужики'\n",
      "12534 ' ужасно'\n",
      "2222 ' тому'\n",
      "12982 ' удиви'\n",
      "751 'лись'\n",
      "289 ' и'\n",
      "2458 ' стали'\n",
      "16136 ' просить'\n",
      "6092 ' лу'\n",
      "299 'то'\n",
      "1427 'ню'\n",
      "50260 '_kw_'\n",
      "1215 ' вот'\n",
      "282 ' в'\n",
      "3709 ' одном'\n",
      "12825 ' селе'\n",
      "3837 ' увидел'\n",
      "492 ' он'\n",
      "19588 ' толпу'\n",
      "31510 ' мужиков'\n",
      "329 ' у'\n",
      "14685 ' избы'\n",
      "50260 '_kw_'\n",
      "510 ' —'\n",
      "329 ' у'\n",
      "703 ' меня'\n",
      "2567 ' таких'\n",
      "8072 ' дура'\n",
      "523 'ков'\n",
      "764 ' еще'\n",
      "1269 ' много'\n",
      "334 ' по'\n",
      "2182 ' бел'\n",
      "272 'у'\n",
      "24612 ' свету'\n",
      "50260 '_kw_'\n",
      "12062 ' прибе'\n",
      "1646 'жал'\n",
      "50261 '_endkw_'\n",
      "50263 '_i_'\n",
      "773 ' им'\n",
      "3348 ' одним'\n",
      "322 ' не'\n",
      "2028 ' мил'\n",
      "520 ' ли'\n",
      "453 ' так'\n",
      "428 ' как'\n",
      "417 ' я'\n",
      "20019 ' бары'\n",
      "301 'ш'\n",
      "19223 'ням'\n",
      "774 ' гол'\n",
      "525 'ая'\n",
      "3858 ' попа'\n",
      "5 '!'\n",
      "203 '\\n'\n",
      "225 ' '\n",
      "372 ' \\n'\n",
      "2424 '  '\n",
      "459 ' И'\n",
      "785 ' когда'\n",
      "1588 ' уж'\n",
      "2152 ' народ'\n",
      "19060 ' собрался'\n",
      "294 ' к'\n",
      "8744 ' дому'\n",
      "19941 ' подка'\n",
      "2022 'тил'\n",
      "16 ','\n",
      "2438 ' стало'\n",
      "975 ' ему'\n",
      "4750 ' видно'\n",
      "374 ' что'\n",
      "1992 ' весь'\n",
      "914 ' дом'\n",
      "2012 ' ха'\n",
      "10965 'оти'\n",
      "695 'чно'\n",
      "16307 ' кругом'\n",
      "2053 ' тол'\n",
      "293 'п'\n",
      "847 'ится'\n",
      "515 ' но'\n",
      "687 ' только'\n",
      "1176 ' один'\n",
      "1135 ' человек'\n",
      "411 ' из'\n",
      "15006 ' толпы'\n",
      "9820 ' крикнул'\n",
      "30 ':'\n",
      "478 ' «'\n",
      "4182 'Как'\n",
      "485 'ие'\n",
      "475 ' все'\n",
      "17 '-'\n",
      "2767 'таки'\n",
      "387 ' вы'\n",
      "45268 ' молодцы'\n",
      "4271 '!»'\n",
      "376 ' -'\n",
      "1209 ' всех'\n",
      "16638 ' аж'\n",
      "1188 ' перед'\n",
      "742 'ник'\n",
      "435 'ло'\n",
      "1974 ' вместе'\n",
      "477 ' со'\n",
      "1514 ' всем'\n",
      "21041 ' народом'\n",
      "581 ' да'\n",
      "519 ' до'\n",
      "924 ' того'\n",
      "3247 ' лица'\n",
      "2710 ' какие'\n",
      "898 ' были'\n",
      "8452 ' изум'\n",
      "4490 'ленные'\n",
      "3367 ' гля'\n",
      "3306 'нули'\n",
      "18 '.'\n",
      "3026 ' Там'\n",
      "575 ' же'\n",
      "16805 ' толпа'\n",
      "5478 ' народа'\n",
      "19918 ' показалась'\n",
      "11696 ' народу'\n",
      "2200 ' своим'\n",
      "10325 ' именем'\n",
      "28605 ' болот'\n",
      "536 'ным'\n",
      "1851 ' какой'\n",
      "537 ' ни'\n",
      "912 ' есть'\n",
      "31 ';'\n",
      "1085 ' кто'\n",
      "424 ' под'\n",
      "743 'ив'\n",
      "811 'ился'\n",
      "2184 ' этим'\n",
      "18146 ' глазам'\n",
      "2353 ' стоит'\n",
      "3609 ' топ'\n",
      "283 'ч'\n",
      "484 'ется'\n",
      "2364 ' около'\n",
      "1129 ' них'\n",
      "3622 ' ря'\n",
      "296 'б'\n",
      "2344 'иной'\n",
      "5729 ' рот'\n",
      "353 'ми'\n",
      "3082 'стр'\n",
      "2710 ' какие'\n",
      "1405 ' свет'\n",
      "1886 'ятся'\n",
      "1264 ' глаза'\n",
      "9523 ' стоят'\n",
      "697 ' их'\n",
      "11753 ' самих'\n",
      "2521 ' будто'\n",
      "373 ' бы'\n",
      "11272 ' ждут'\n",
      "5332 ' чтоб'\n",
      "42287 ' броситься'\n",
      "31488 ' толпой'\n",
      "9729 ' навстречу'\n",
      "31120 ' великому'\n",
      "42360 ' бари'\n",
      "392 'ну'\n",
      "428 ' как'\n",
      "2471 ' вдруг'\n",
      "8493 ' видит'\n",
      "2521 ' будто'\n",
      "583 ' сам'\n",
      "8478 ' хозяин'\n",
      "1560 ' теле'\n",
      "607 'ги'\n",
      "40750 ' выбе'\n",
      "5779 'гает'\n",
      "2705 ' хва'\n",
      "3407 'тая'\n",
      "3312 ' слу'\n",
      "877 'гу'\n",
      "43474 ' пана'\n",
      "760 ' гор'\n",
      "14916 'бат'\n",
      "346 'ого'\n",
      "2024 ' которого'\n",
      "16560 ' держит'\n",
      "7746 ' маленький'\n",
      "8339 ' мальчик'\n",
      "749 ' без'\n",
      "44331 ' шапки'\n",
      "1548 ' потому'\n",
      "5843 ' пан'\n",
      "8685 ' блед'\n",
      "268 'н'\n",
      "26718 'олицы'\n",
      "295 'й'\n",
      "17290 ' бежал'\n",
      "18007 ' бегу'\n",
      "25442 ' бежит'\n",
      "30575 ' бегом'\n",
      "21780 ' кричит'\n",
      "16088 ' сыну'\n",
      "4453 ' своему'\n",
      "15955 ' стыдно'\n",
      "33875 ' стыда'\n",
      "21798 ' нету'\n",
      "1696 ' ведь'\n",
      "1314 ' сейчас'\n",
      "30040 ' плачет'\n",
      "5198 ' ибо'\n",
      "1381 ' надо'\n",
      "639 ' было'\n",
      "10790 ' бежать'\n",
      "3136 ' ну'\n",
      "2189 ' коли'\n",
      "1441 ' теперь'\n",
      "2955 ' нельзя'\n",
      "9227 ' спасти'\n",
      "5794 ' сына'\n",
      "360 ' от'\n",
      "43802 ' плена'\n",
      "655 '…'\n",
      "1492 ' .'\n",
      "470 '»'\n",
      "985 ' Как'\n",
      "481 ' это'\n",
      "10677 ' жаль'\n",
      "26942 ' »'\n",
      "4507 ' подумал'\n",
      "12387 ' великий'\n",
      "24874 ' богаты'\n",
      "1666 'рь'\n",
      "510 ' —'\n",
      "1345 ' Мы'\n",
      "821 ' пла'\n",
      "1313 'чем'\n",
      "290 ' о'\n",
      "1169 ' нем'\n",
      "656 ' мы'\n",
      "8776 ' уте'\n",
      "669 'ша'\n",
      "339 'ем'\n",
      "1634 ' своих'\n",
      "2533 ' детей'\n",
      "21023 '….'\n",
      "471 '«'\n",
      "563 'С'\n",
      "1088 'каза'\n",
      "315 'но'\n",
      "1337 ' вам'\n",
      "629 ' Я'\n",
      "36742 ' бедный'\n",
      "7613 ' господин'\n",
      "2218 ' мой'\n",
      "9301 ' князь'\n",
      "4944 ' Андрей'\n",
      "30766 ' Андреевич'\n",
      "7808 ' убит'\n",
      "879 'ый'\n",
      "48214 ' горем'\n",
      "8282 ' старый'\n",
      "1029 ' который'\n",
      "23909 ' умирает'\n",
      "371 ' при'\n",
      "784 ' мне'\n",
      "11420 ' …'\n",
      "264 'а'\n",
      "3153 ' говорил'\n",
      "1003 ' нет'\n",
      "1379 ' наш'\n",
      "12082 ' царь'\n",
      "36956 ' батюшка'\n",
      "1544 ' жив'\n",
      "6088 ' остался'\n",
      "1965 ' совсем'\n",
      "726 ' уже'\n",
      "1642 ' дома'\n",
      "3809 ' отец'\n",
      "1500 ' тот'\n",
      "939 ' очень'\n",
      "6392 ' тяжело'\n",
      "32842 ' болен'\n",
      "7118 ' живет'\n",
      "12416 ' еле'\n",
      "10974 ' живой'\n",
      "4411 ' мать'\n",
      "3421 ' кня'\n",
      "30333 'жна'\n",
      "25486 ' Анастасия'\n",
      "14357 ' умерла'\n",
      "5548 ' недавно'\n",
      "6304 ' дочь'\n",
      "722 ' ее'\n",
      "1122 ' там'\n",
      "678 ' она'\n",
      "8838 ' похоро'\n",
      "15786 'нена'\n",
      "906 ' была'\n",
      "45318 ' княгиня'\n",
      "9409 ' Ольга'\n",
      "11240 ' жила'\n",
      "1715 ' почти'\n",
      "5004 ' месяц'\n",
      "988 ' где'\n",
      "1655 ' тогда'\n",
      "685 ' был'\n",
      "39218 ' похоронен'\n",
      "23115 ' государь'\n",
      "32464 ' московский'\n",
      "9543 ' Юрий'\n",
      "16420 ' Александрович'\n",
      "5557 ' мама'\n",
      "3476 ' моя'\n",
      "9502 ' сестра'\n",
      "28585 ' Елизавета'\n",
      "25615 ' уехала'\n",
      "3660 ' жить'\n",
      "437 ' ко'\n",
      "2799 ' вну'\n",
      "4762 'чке'\n",
      "1450 ' ей'\n",
      "1223 ' всего'\n",
      "10384 ' девять'\n",
      "986 ' лет'\n",
      "2158 ' назад'\n",
      "3826 ' умер'\n",
      "332 'ла'\n",
      "39301 ' графиня'\n",
      "10261 ' Елена'\n",
      "22891 ' Ивановна'\n",
      "43200 ' скончалась'\n",
      "13796 ' племян'\n",
      "2016 'ница'\n",
      "41035 ' императрицы'\n",
      "15073 ' Марии'\n",
      "15090 ' Федоров'\n",
      "318 'ны'\n",
      "25180 ' жива'\n",
      "8824 ' осталась'\n",
      "13801 ' бабушка'\n",
      "2588 ' которую'\n",
      "15220 ' звали'\n",
      "48852 ' королевой'\n",
      "8401 ' русской'\n",
      "25305 ' королевы'\n",
      "17591 ' родилась'\n",
      "567 ' во'\n",
      "808 ' время'\n",
      "1772 ' кре'\n",
      "2005 'щения'\n",
      "13147 ' императора'\n",
      "6816 ' Александра'\n",
      "907 ' I'\n",
      "473 ' ('\n",
      "2692 '18'\n",
      "11498 '54'\n",
      "443 '–'\n",
      "1618 '19'\n",
      "8492 '07'\n",
      "13 ')'\n",
      "971 ' потом'\n",
      "5015 ' наслед'\n",
      "5656 'овала'\n",
      "11538 ' император'\n",
      "2818 'скому'\n",
      "33846 ' двору'\n",
      "22987 ' Российская'\n",
      "29870 ' империя'\n",
      "2742 ' начала'\n",
      "7812 ' войну'\n",
      "9601 ' началась'\n",
      "6888 ' война'\n",
      "1279 ' против'\n",
      "5306 ' Германии'\n",
      "3771 ' Россия'\n",
      "37818 ' вступила'\n",
      "2566 ' побед'\n",
      "19517 'онос'\n",
      "549 'ная'\n",
      "15871 ' Германия'\n",
      "46621 ' проиграла'\n",
      "38552 ' победила'\n",
      "1175 ' России'\n",
      "7356 ' получила'\n",
      "13474 ' Финлян'\n",
      "4879 'дия'\n",
      "491 ' 2'\n",
      "3990 ' марта'\n",
      "13959 ' 1918'\n",
      "779 ' года'\n",
      "3875 ' СССР'\n",
      "12215 ' присоедин'\n",
      "2582 'яет'\n",
      "39835 ' Карели'\n",
      "306 'ю'\n",
      "1332 ' город'\n",
      "24064 ' Мурман'\n",
      "328 'ск'\n",
      "3703 ' 23'\n",
      "3598 ' мая'\n",
      "11355 ' 1945'\n",
      "336 ' г'\n",
      "2309 ' мир'\n",
      "451 'ный'\n",
      "4957 ' договор'\n",
      "29292 ' подписан'\n",
      "45649 ' союзниками'\n",
      "4433 ' Российской'\n",
      "4551 ' Федерации'\n",
      "5524 ' подпис'\n",
      "2625 'ывают'\n",
      "2145 ' США'\n",
      "30676 ' Договор'\n",
      "381 ' об'\n",
      "17147 ' окончании'\n",
      "2793 ' войны'\n",
      "15739 ' вступил'\n",
      "4199 ' Совет'\n",
      "1703 'ским'\n",
      "10134 ' Союз'\n",
      "304 'ом'\n",
      "18566 ' принят'\n",
      "41175 ' мирный'\n",
      "23828 ' протокол'\n",
      "27875 ' подписали'\n",
      "18211 ' Франция'\n",
      "39991 ' Подпис'\n",
      "363 'ан'\n",
      "8579 ' Великобрита'\n",
      "5885 'нией'\n",
      "34604 ' рати'\n",
      "3248 'фика'\n",
      "2206 'цией'\n",
      "29292 ' подписан'\n",
      "31469 ' Германией'\n",
      "14900 ' подписал'\n",
      "44731 ' Францией'\n",
      "9565 ' подписа'\n",
      "324 'на'\n",
      "7688 ' Япо'\n",
      "1525 'нием'\n",
      "5897 ' заключ'\n",
      "1504 'ён'\n",
      "5508 ' союз'\n",
      "448 'ные'\n",
      "37390 ' договоры'\n",
      "22654 ' открыты'\n",
      "_start_  лутонюшка _kw_  уронила его на загнетку и тут превеликим голосом закричала и завопила _kw_  привязали они в воротах хомут и палками вгоняют в этот хомут лошадь _kw_  а сама то и дело ходит с ложкою в погреб за сметаной _kw_  мужики ужасно тому удивились и стали просить лутоню _kw_  вот в одном селе увидел он толпу мужиков у избы _kw_  — у меня таких дураков еще много по белу свету _kw_  прибежал _endkw_ _i_  им одним не мил ли так как я барышням голая попа!\n",
      "  \n",
      "   И когда уж народ собрался к дому подкатил, стало ему видно что весь дом хаотично кругом толпится но только один человек из толпы крикнул: «Какие все-таки вы молодцы!» - всех аж передникло вместе со всем народом да до того лица какие были изумленные глянули. Там же толпа народа показалась народу своим именем болотным какой ни есть; кто подивился этим глазам стоит топчется около них рябиной ротмистр какие светятся глаза стоят их самих будто бы ждут чтоб броситься толпой навстречу великому барину как вдруг видит будто сам хозяин телеги выбегает хватая слугу пана горбатого которого держит маленький мальчик без шапки потому пан бледнолицый бежал бегу бежит бегом кричит сыну своему стыдно стыда нету ведь сейчас плачет ибо надо было бежать ну коли теперь нельзя спасти сына от плена… .» Как это жаль » подумал великий богатырь — Мы плачем о нем мы утешаем своих детей….«Сказано вам Я бедный господин мой князь Андрей Андреевич убитый горем старый который умирает при мне …а говорил нет наш царь батюшка жив остался совсем уже дома отец тот очень тяжело болен живет еле живой мать княжна Анастасия умерла недавно дочь ее там она похоронена была княгиня Ольга жила почти месяц где тогда был похоронен государь московский Юрий Александрович мама моя сестра Елизавета уехала жить ко внучке ей всего девять лет назад умерла графиня Елена Ивановна скончалась племянница императрицы Марии Федоровны жива осталась бабушка которую звали королевой русской королевы родилась во время крещения императора Александра I (1854–1907) потом наследовала императорскому двору Российская империя начала войну началась война против Германии Россия вступила победоносная Германия проиграла победила России получила Финляндия 2 марта 1918 года СССР присоединяет Карелию город Мурманск 23 мая 1945 г мирный договор подписан союзниками Российской Федерации подписывают США Договор об окончании войны вступил Советским Союзом принят мирный протокол подписали Франция Подписан Великобританией ратификацией подписан Германией подписал Францией подписана Японием заключён союзные договоры открыты\n",
      "--------------------------------------------------\n",
      "_start_ лутонюшка_kw_ уронила его на загнетку и тут превеликим голосом закричала и завопила_kw_ привязали они в воротах хомут и палками вгоняют в этот хомут лошадь_kw_ а сама то и дело ходит с ложкою в погреб за сметаной_kw_ мужики ужасно тому удивились и стали просить лутоню_kw_ вот в одном селе увидел он толпу мужиков у избы_kw_ — у меня таких дураков еще много по белу свету_kw_ прибежал_endkw__i_ им одним не мил ли так как я барышням голая попа!\n",
      "  \n",
      "   И когда уж народ собрался к дому подкатил, стало ему видно что весь дом хаотично кругом толпится но только один человек из толпы крикнул: «Какие все-таки вы молодцы!» - всех аж передникло вместе со всем народом да до того лица какие были изумленные глянули. Там же толпа народа показалась народу своим именем болотным какой ни есть; кто подивился этим глазам стоит топчется около них рябиной ротмистр какие светятся глаза стоят их самих будто бы ждут чтоб броситься толпой навстречу великому барину как вдруг видит будто сам хозяин телеги выбегает хватая слугу пана горбатого которого держит маленький мальчик без шапки потому пан бледнолицый бежал бегу бежит бегом кричит сыну своему стыдно стыда нету ведь сейчас плачет ибо надо было бежать ну коли теперь нельзя спасти сына от плена… .» Как это жаль » подумал великий богатырь — Мы плачем о нем мы утешаем своих детей….«Сказано вам Я бедный господин мой князь Андрей Андреевич убитый горем старый который умирает при мне …а говорил нет наш царь батюшка жив остался совсем уже дома отец тот очень тяжело болен живет еле живой мать княжна Анастасия умерла недавно дочь ее там она похоронена была княгиня Ольга жила почти месяц где тогда был похоронен государь московский Юрий Александрович мама моя сестра Елизавета уехала жить ко внучке ей всего девять лет назад умерла графиня Елена Ивановна скончалась племянница императрицы Марии Федоровны жива осталась бабушка которую звали королевой русской королевы родилась во время крещения императора Александра I (1854–1907) потом наследовала императорскому двору Российская империя начала войну началась война против Германии Россия вступила победоносная Германия проиграла победила России получила Финляндия 2 марта 1918 года СССР присоединяет Карелию город Мурманск 23 мая 1945 г мирный договор подписан союзниками Российской Федерации подписывают США Договор об окончании войны вступил Советским Союзом принят мирный протокол подписали Франция Подписан Великобританией ратификацией подписан Германией подписал Францией подписана Японием заключён союзные договоры открыты\n"
     ]
    }
   ],
   "source": [
    "str_rep = []\n",
    "end_tok = encoder.convert_tokens_to_ids('_end_')\n",
    "\n",
    "for token in sample_output[0]:\n",
    "    print(token.item(), repr(decoder.decode([ encoder.convert_ids_to_tokens(token.item(), skip_special_tokens=True)])))\n",
    "    if token.item() == end_tok : #or token.item() == 0:# or x.item() == end_idx:\n",
    "        break        \n",
    "    str_rep.append(encoder.convert_ids_to_tokens(token.item()))\n",
    "\n",
    "str_rep = encoder.convert_tokens_to_string(str_rep)\n",
    "\n",
    "# This makes sure rouge scorers doesn't complain about no sentences\n",
    "if not str_rep:\n",
    "    str_rep = \"unk.\"\n",
    "elif \".\" not in str_rep:\n",
    "    str_rep += \".\"\n",
    "\n",
    "print(encoder.decode(sample_output[0], skip_special_tokens=False, clean_up_tokenization_spaces=False))\n",
    "print(\"-\"*50)\n",
    "print(str_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_start_ лутонюшка_kw_ уронила его на загнетку и тут превеликим голосом закричала и завопила_kw_ привязали они в воротах хомут и палками вгоняют в этот хомут лошадь_kw_ а сама то и дело ходит с ложкою в погреб за сметаной_kw_ мужики ужасно тому удивились и стали просить лутоню_kw_ вот в одном селе увидел он толпу мужиков у избы_kw_ — у меня таких дураков еще много по белу свету_kw_ прибежал_endkw__i_ однако лото-ну я им сказал, что без помощи бузотерпцев не обойдется.\\nВыйдя из ворот хутора вижу мужик стоит перед своей коновязи машет ему рукой хочет побить и все остальные кричат: \\n -Погоди ты че молчишь!  Ну? \\n А тот говорит да што делать... ну взял коня под узду будто бы кнутом стал бить и пошел дальше как вдруг видит какой был человек встал возле изгороди так стоял долго смотрел почему подошел ко мне но только спросил \"Как зовут\" может быть Я отвечаю хорошо знаю русский язык слова \"справедливость\", очень неплохо говорю спасибо думаю можно выучить их себе самому или вам пригодится если хотите знать чего нибудь напишите пожалуйста о том кто такой Толстой Федор Михайлович Достоевский писатель Владимир Владимирович Лермонтовский солдат Леонид Ильич Ленин Сталин Брежнев Иосиф Виссарионович Молотов Ворошилова товарищ Ким Ир Сен Вождь Японии Одзаки Пардо Пэй Навуходоносор Черчилль Франклин Рузвельт Джон Кеннеди Маккейн И это уже было когда мы видели теленка который бегал рядом со своим отцом через кукурузный пол я помню там была одна девочка которая сидела напротив него во время еды она пела песни которые были разные люди вокруг нее бегали девушки спрашивали ее какие ей нравятся такие песенки тогда же узнал чем занимается один старик лет 70 наверное тоже играл музыку которую сейчас называют ретро музыкой времени после войны наверно старые пластинки где есть музыка 60х годов песню про танки помните эту девочку звали джейлбэк рэнд вроде раньше любила играть в игры типа \"вприпрыжку прыгай 10 метров\".И эта девчонка танцевала танец живота потом вышла замуж (была замужем) стала актрисой..Дальше начался обычный быт того самого мужика которого называли Робин Гуд Крузо начал ходить пешком до той самой реки чтобы посмотреть развеселую сценушку какую показывали соседи(еще чаще всего говорили \"вот эти двое\")потом стало немного лучше жить надоело слушать радио ,лучше ездить домой смотреть телевизор теперь гораздо больше слушаю книги читаю фильмы смотрю новости--а вообще мультики смотрят почти всегда взрослые'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks_to_str(sample_output[0], encoder)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d3a13fa35461c6d7f441fbb3d93fc4f14860aa527e8f914775d5e57b8364c02"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
