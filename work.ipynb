{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import rouge\n",
    "import codecs\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.model.logger import Logger\n",
    "from src.model.data_full import RawFilesDataset\n",
    "from src.model.loss import ParagraphLoss\n",
    "from src.model.generate_utils import toks_to_str\n",
    "\n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "\n",
    "# from src.model.generate_utils  import generate_paragraph\n",
    "# from src.model.eval_utils import evaluate_doc_model\n",
    "# from src.model.model import GPT2BaseModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.decoders import ByteLevel\n",
    "decoder = ByteLevel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_rep = []\n",
    "end_tok = encoder.convert_tokens_to_ids('_end_')\n",
    "\n",
    "for token in sample_output[0]:\n",
    "    print(token.item(), repr(decoder.decode([ encoder.convert_ids_to_tokens(token.item(), skip_special_tokens=True)])))\n",
    "    if token.item() == end_tok : #or token.item() == 0:# or x.item() == end_idx:\n",
    "        break        \n",
    "    str_rep.append(encoder.convert_ids_to_tokens(token.item()))\n",
    "\n",
    "str_rep = encoder.convert_tokens_to_string(str_rep)\n",
    "\n",
    "# This makes sure rouge scorers doesn't complain about no sentences\n",
    "if not str_rep:\n",
    "    str_rep = \"unk.\"\n",
    "elif \".\" not in str_rep:\n",
    "    str_rep += \".\"\n",
    "\n",
    "print(encoder.decode(sample_output[0], skip_special_tokens=False, clean_up_tokenization_spaces=False))\n",
    "print(\"-\"*50)\n",
    "print(str_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    repeattheta = 1.5\n",
    "    output_attentions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = len(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model = GPT2BaseModel(args, vocab=vocab, n_ctx=config['n_ctx'], gen_len=401, lastidx=encoder.eos_token_id, includeprev=False, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_doc_model(model=doc_model, val_loader=val_loader, text_encoder=encoder, device='cpu', beam=0, gen_len=401, k=0, p=90, save_file='out', max_len=512, gen_dir=None, tgt_dir=None, min_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'w', encoding='utf-8') as f:\n",
    "    json.dump(\"Моя строка\", f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('generated/test.gens.tsv', sep='\\t', header=None, names=['id', 'plot', 'context', 'part', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* для токенизаторов в исходном коде не применяется язык\n",
    "* англ токенизатор предложений лучше делит, к примеру русский не смог разделить 'Король дал за дочкой богатое приданое, наградил зятя большим чином и задал пир на весь мир.\\nЖивут молодые месяц, и два, и три.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Metric, Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = '111 Волшебное кольцо.txt'\n",
    "path = 'dataset/raw'\n",
    "with open(os.path.join(path, story), 'r', encoding='utf-8') as f:\n",
    "    text =  f.read()\n",
    "    text = re.sub('\\.\\.\\.', '.', text)\n",
    "    text = re.sub('—', '-', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_scores(hyps, refs):       \n",
    "    rouge_scorer = rouge.Rouge()\n",
    "    averaged_scores = rouge_scorer.get_scores(hyps, refs, avg=True)\n",
    "    return averaged_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = GPT2Tokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\", add_prefix_space=True)\n",
    "text_encoder.add_special_tokens({'bos_token': '<s>',                                     \n",
    "                                    'eos_token': '</s>',\n",
    "                                    'additional_special_tokens': ['[SEP]', '_kw_', '_endkw_']\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('savedir/s_all_nodiscourse_kw/checkpoints/checkpoint.pt', 'rb') as f:\n",
    "    model = torch.load(f, map_location=torch.device('cpu'))\n",
    "\n",
    "with open('savedir/s_all_nodiscourse_kw/test_dataset', 'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = RawFilesDataset(test, text_encoder, 2048, n_ctx=70)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "batch = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    print(list( batch['mask'][0,0:70].numpy() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['sample'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.additional_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(batch['mask'][0,0:70].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch(batch: dict, text_encoder: GPT2Tokenizer)-> tuple:\n",
    "    septok = text_encoder.convert_tokens_to_ids('[SEP]')\n",
    "    endtok = text_encoder.eos_token_id\n",
    "    input_ids, mask = batch['sample'], batch['mask']\n",
    "\n",
    "    sep_idx = torch.where(input_ids[0] == septok)[0].item()\n",
    "    eos_idx = torch.where(input_ids[0] == endtok)[0].item()\n",
    "    context = input_ids[:, :sep_idx+1]\n",
    "    target_txt = input_ids[:, sep_idx+1:eos_idx+1]\n",
    "\n",
    "    context_txt = text_encoder.decode(context[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    refs = text_encoder.decode(target_txt[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    sample_output = model.generate(\n",
    "                                    context, \n",
    "                                    # attention_mask=mask,\n",
    "                                    max_length=512, \n",
    "                                    do_sample=True,\n",
    "                                    num_beams = 20,  # https://arxiv.org/pdf/2108.03502.pdf \n",
    "                                    top_p=0.95, # https://arxiv.org/pdf/2108.03502.pdf \n",
    "                                    top_k=3, # https://arxiv.org/pdf/2108.03502.pdf\n",
    "                                    eos_token_id=endtok,\n",
    "                                    bos_token_id=text_encoder.bos_token_id,\n",
    "                                    decoder_start_token_id = septok,\n",
    "                                    min_length = 100,\n",
    "                                    num_return_sequences=1, \n",
    "                                    temperature=1.0, # https://arxiv.org/pdf/2108.03502.pdf\n",
    "                                    repetition_penalty=2.0,  # https://arxiv.org/pdf/2108.03502.pdf\n",
    "                                    no_repeat_ngram_size=3, # https://arxiv.org/pdf/2108.03502.pdf\n",
    "                                    forced_eos_token_id = endtok,\n",
    "                                    early_stopping=True  # https://arxiv.org/pdf/2108.03502.pdf\n",
    "                                )\n",
    "    hyps = text_encoder.decode(sample_output[0][sep_idx+1:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    rouge_score = rouge_scores(hyps, refs)\n",
    "    return context_txt, refs, hyps, rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_text(text: str=\"\") -> str:\n",
    "    return text.replace('\\r\\n',' ').replace('\\n',' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_evaluate_result(data: tuple, path: str=''):\n",
    "    columns = ['context', 'refs', 'hyps', 'ROUGE-1 F1', 'ROUGE-2 F1', 'ROUGE-L F1']\n",
    "    assert len(columns) == len(data[0])    \n",
    "    with open(os.path.join(path, 'evaluate_results.csv'), 'w', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f, delimiter='|', lineterminator='\\n')\n",
    "        writer.writerow(columns)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'savedir/s_all_nodiscourse_kw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "context, refs, hyps, score = evaluate_batch(batch, text_encoder)\n",
    "data.append( (context, flat_text(refs), flat_text(hyps), score['rouge-1']['f'], score['rouge-2']['f'], score['rouge-l']['f']) )\n",
    "write_evaluate_result(data, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(path, 'evaluate_results.csv'), sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ROUGE-L F1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(path, 'generated_stories.csv'), sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.refs.item())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d3a13fa35461c6d7f441fbb3d93fc4f14860aa527e8f914775d5e57b8364c02"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
