{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edbon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\edbon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import rouge\n",
    "import codecs\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.model.logger import Logger\n",
    "from src.model.data_full import RawFilesDataset\n",
    "from src.model.loss import ParagraphLoss\n",
    "from src.model.generate_utils import toks_to_str\n",
    "\n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# from src.model.generate_utils  import generate_paragraph\n",
    "# from src.model.eval_utils import evaluate_doc_model\n",
    "# from src.model.model import GPT2BaseModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.decoders import ByteLevel\n",
    "decoder = ByteLevel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_rep = []\n",
    "end_tok = encoder.convert_tokens_to_ids('_end_')\n",
    "\n",
    "for token in sample_output[0]:\n",
    "    print(token.item(), repr(decoder.decode([ encoder.convert_ids_to_tokens(token.item(), skip_special_tokens=True)])))\n",
    "    if token.item() == end_tok : #or token.item() == 0:# or x.item() == end_idx:\n",
    "        break        \n",
    "    str_rep.append(encoder.convert_ids_to_tokens(token.item()))\n",
    "\n",
    "str_rep = encoder.convert_tokens_to_string(str_rep)\n",
    "\n",
    "# This makes sure rouge scorers doesn't complain about no sentences\n",
    "if not str_rep:\n",
    "    str_rep = \"unk.\"\n",
    "elif \".\" not in str_rep:\n",
    "    str_rep += \".\"\n",
    "\n",
    "print(encoder.decode(sample_output[0], skip_special_tokens=False, clean_up_tokenization_spaces=False))\n",
    "print(\"-\"*50)\n",
    "print(str_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    repeattheta = 1.5\n",
    "    output_attentions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = len(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model = GPT2BaseModel(args, vocab=vocab, n_ctx=config['n_ctx'], gen_len=401, lastidx=encoder.eos_token_id, includeprev=False, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_doc_model(model=doc_model, val_loader=val_loader, text_encoder=encoder, device='cpu', beam=0, gen_len=401, k=0, p=90, save_file='out', max_len=512, gen_dir=None, tgt_dir=None, min_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'w', encoding='utf-8') as f:\n",
    "    json.dump(\"Моя строка\", f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('generated/test.gens.tsv', sep='\\t', header=None, names=['id', 'plot', 'context', 'part', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* для токенизаторов в исходном коде не применяется язык\n",
    "* англ токенизатор предложений лучше делит, к примеру русский не смог разделить 'Король дал за дочкой богатое приданое, наградил зятя большим чином и задал пир на весь мир.\\nЖивут молодые месяц, и два, и три.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Metric, Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = '111 Волшебное кольцо.txt'\n",
    "path = 'dataset/raw'\n",
    "with open(os.path.join(path, story), 'r', encoding='utf-8') as f:\n",
    "    text =  f.read()\n",
    "    text = re.sub('\\.\\.\\.', '.', text)\n",
    "    text = re.sub('—', '-', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_scores(hyps, refs):       \n",
    "    rouge_scorer = rouge.Rouge()\n",
    "    averaged_scores = rouge_scorer.get_scores(hyps, refs, avg=True)\n",
    "    return averaged_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder = GPT2Tokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\", add_prefix_space=True)\n",
    "text_encoder.add_special_tokens({'bos_token': '<s>',                                     \n",
    "                                    'eos_token': '</s>',\n",
    "                                    'additional_special_tokens': ['[SEP]', '_kw_', '_endkw_']\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('savedir/savedir/s_all_nodiscourse/checkpoints/checkpoint.pt', 'rb') as f:\n",
    "    model = torch.load(f, map_location=torch.device('cpu'))\n",
    "\n",
    "with open('savedir/savedir/s_all_nodiscourse/test_dataset', 'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = RawFilesDataset(test, text_encoder, 2048, n_ctx=70)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "batch = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "septok = text_encoder.convert_tokens_to_ids('[SEP]')\n",
    "endtok = text_encoder.eos_token_id\n",
    "input_ids, mask = batch['sample'], batch['mask']\n",
    "\n",
    "sep_idx = torch.where(input_ids[0] == septok)[0].item()\n",
    "eos_idx = torch.where(input_ids[0] == endtok)[0].item()\n",
    "context = input_ids[:, :sep_idx+1]\n",
    "target_txt = input_ids[:, sep_idx+1:eos_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_txt = text_encoder.decode(context[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "# context_txt = re.sub('—|\\.\\.\\.', ' ', context_txt)\n",
    "# context = text_encoder.encode(context_txt, return_tensors='pt')\n",
    "\n",
    "refs = text_encoder.decode(target_txt[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context_txt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> царь тотчас снял кольцо _kw_ кот васька говорит _kw_ кот взял кольцо _kw_ днем королевна носит кольцо _kw_ кот васька сел собаке _kw_ дня сидел кот васька _kw_ нам помогут кольцо найти _kw_ достать чудодейное кольцо _kw_ добывать чудодейное кольцо _kw_ кот васька цап _kw_ задумали кот васька _kw_ чудодейное кольцо _endkw_ [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(context_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = model.generate(\n",
    "    context, \n",
    "    # attention_mask=mask,\n",
    "    max_length=512, \n",
    "    do_sample=True,\n",
    "    num_beams = 20,  # https://arxiv.org/pdf/2108.03502.pdf \n",
    "    top_p=0.95, # https://arxiv.org/pdf/2108.03502.pdf \n",
    "    top_k=3, # https://arxiv.org/pdf/2108.03502.pdf\n",
    "    eos_token_id=endtok,\n",
    "    bos_token_id=text_encoder.bos_token_id,\n",
    "    decoder_start_token_id = septok,\n",
    "    min_length = 100,\n",
    "    num_return_sequences=1, \n",
    "    temperature=1.0, # https://arxiv.org/pdf/2108.03502.pdf\n",
    "    repetition_penalty=2.0,  # https://arxiv.org/pdf/2108.03502.pdf\n",
    "    no_repeat_ngram_size=3, # https://arxiv.org/pdf/2108.03502.pdf\n",
    "    forced_eos_token_id = endtok,\n",
    "    early_stopping=True  # https://arxiv.org/pdf/2108.03502.pdf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps1 = text_encoder.decode(sample_output[0][sep_idx+1:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rouge_scores(hyps1, refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hyps1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d3a13fa35461c6d7f441fbb3d93fc4f14860aa527e8f914775d5e57b8364c02"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
