{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "from nltk.corpus import stopwords\n",
    "from rake_nltk import Rake\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# promt with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_config = {'pad_len': 1024, \n",
    "                'train_batch_size': 2,\n",
    "                'lr': 6.25e-5,\n",
    "                'chk_path': 'savedir/gpt3full'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50262, 1024)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder = GPT2Tokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "text_encoder.add_special_tokens({'bos_token':'_start_',\n",
    "                                    'cls_token':'_classify_',\n",
    "                                    'eos_token':'_end_',\n",
    "                                    'additional_special_tokens': ['_kw_', '_endkw_']\n",
    "                                })\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(text_encoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(path: str, output_path: str, file_names: list, output_file: str, topK: int):\n",
    "    sentences_to_write = []\n",
    "    sentences_to_write.append(\"[KEYWORDS]\\t[TEXT]\\n\")\n",
    "    \n",
    "    f_out = open(os.path.join(output_path, output_file), 'w', encoding='utf-8')\n",
    "\n",
    "    for file in file_names:\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            text = text.replace('\\u2003', ' ')\n",
    "            text = text.replace('\\n', ' ').replace('  ', ' ').strip()\n",
    "\n",
    "            try:\n",
    "                r = Rake(language='russian', stopwords=stopwords.words())\n",
    "                r.extract_keywords_from_text(text)\n",
    "                top_features = r.get_ranked_phrases()\n",
    "                if len(top_features) > topK:\n",
    "                    top_features = top_features[:topK]\n",
    "\n",
    "            except Exception:\n",
    "                print(text[:50])\n",
    "                continue\n",
    "\n",
    "            keywordsSTR = \"[SEP]\".join([kw.strip() for kw in top_features if len(kw.split()) > 2]).strip()\n",
    "\n",
    "            title = re.sub(\"[^А-Яа-я]\" , \" \", file.split('.')[0]).strip()        \n",
    "\n",
    "            if len(title) > 2:\n",
    "                title = title.lower().strip()\n",
    "                keywordsSTR = title + '[SEP]' + keywordsSTR\n",
    "                if len(keywordsSTR.split(' ')) > 100:\n",
    "                    keywordsSTR = ' '.join(keywordsSTR.split(' ')[:100]).strip()\n",
    "\n",
    "            sentences_to_write.append(keywordsSTR + '\\t' + text + '\\n')\n",
    "\n",
    "    f_out.writelines(sentences_to_write)\n",
    "    f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dataset/raw/'\n",
    "output_path = 'dataset/full/'\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "input_files = os.listdir(path)\n",
    "\n",
    "[os.remove(os.path.join(output_path, out_file)) for out_file in os.listdir(output_path)]\n",
    "\n",
    "# 90/5/5\n",
    "train, testval = train_test_split(input_files, test_size=0.1)\n",
    "val, test = train_test_split(testval, test_size=0.5)\n",
    "\n",
    "preprocess_texts(path, output_path, train, 'train_full', 5)\n",
    "preprocess_texts(path, output_path, val, 'val_full', 5)\n",
    "preprocess_texts(path, output_path, test, 'test_full', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_file: str, \n",
    "                 tokenizer: GPT2Tokenizer, \n",
    "                 pad_len: int,\n",
    "                 ):\n",
    "\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = f.readlines()\n",
    "\n",
    "        self.data = []\n",
    "        for d in range(1, len(data)):\n",
    "            t = data[d].decode(\"utf-8\", \"ignore\").strip().split('\\t')\n",
    "            if len(t) == 2:\n",
    "                self.data.append(t)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_len = pad_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, indx):\n",
    "        context, target_txt = self.data[indx]        \n",
    "\n",
    "        context = self.tokenizer.encode(context)\n",
    "        target_txt = self.tokenizer.encode(target_txt)\n",
    "        \n",
    "        clstok = self.tokenizer.cls_token_id\n",
    "        keytok = self.tokenizer.convert_tokens_to_ids('_kw_')\n",
    "        endkeytok = self.tokenizer.convert_tokens_to_ids('_endkw_')\n",
    "\n",
    "        sample = [self.tokenizer.bos_token_id] + \\\n",
    "                 [keytok] + context + [endkeytok] + \\\n",
    "                 [clstok] + target_txt + \\\n",
    "                 [self.tokenizer.eos_token_id]\n",
    "        \n",
    "        if len(sample) <= self.pad_len:            \n",
    "            mask = [1] * len(sample) + [0] * (self.pad_len - len(sample))\n",
    "            label = sample + [-100] * (self.pad_len - len(sample))\n",
    "            sample = sample + [self.tokenizer.bos_token_id] * (self.pad_len - len(sample))\n",
    "        else:\n",
    "            sample = sample[:self.pad_len]\n",
    "            sample[-1] = self.tokenizer.eos_token_id\n",
    "            mask = [1] * len(sample) + [0] * (self.pad_len - len(sample))\n",
    "            label = sample + [-100] * (self.pad_len - len(sample))\n",
    "\n",
    "        sample = torch.tensor(sample)\n",
    "        mask = torch.tensor(mask)\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return {\n",
    "            'sample': sample, \n",
    "            'mask': mask, \n",
    "            'label': label\n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FullDataset(os.path.join(output_path, 'train_full'), text_encoder, params_config['pad_len'])\n",
    "train_loader = DataLoader(train_dataset, params_config['train_batch_size'], shuffle=True)\n",
    "\n",
    "val_dataset = FullDataset(os.path.join(output_path, 'val_full'), text_encoder, params_config['pad_len'])\n",
    "val_loader = DataLoader(val_dataset, params_config['train_batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=params_config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, test_loader, optimizer, epoch_num, device, log_interval=10):\n",
    "    losses = []\n",
    "    avg_loss = []\n",
    "    step = 1\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, mask, label = batch['sample'], batch['mask'], batch['label']\n",
    "        input_ids = input_ids.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "        outputs = model(input_ids, attention_mask=mask, labels=label)\n",
    "        loss, _ = outputs[:2]\n",
    "        avg_loss.append(loss.detach().item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % log_interval == 0:\n",
    "            val_loss = sum(avg_loss) / len(avg_loss)\n",
    "            losses.append(val_loss)\n",
    "            avg_loss = []            \n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \n",
    "            params_config['chk_path'])         \n",
    "        step += 1\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    ep_losses = train_epoch(model, train_loader, val_loader, optimizer, epoch, device, log_interval=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.pad_token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50258"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.cls_token_id"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d3a13fa35461c6d7f441fbb3d93fc4f14860aa527e8f914775d5e57b8364c02"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('tf': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
